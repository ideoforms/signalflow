{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SignalFlow","text":"<p>Warning</p> <p>This documentation is a work-in-progress and may have sections that are missing or incomplete.</p> <p>SignalFlow is an audio synthesis framework whose goal is to make it quick and intuitive to explore complex sonic ideas. It has a simple and consistent Python API, allowing for rapid prototyping in Jupyter, PyCharm, or on the command-line. It comes with over 100 of built-in node classes for creative exploration.</p> <p>Its core is implemented in C++11, with cross-platform hardware acceleration.  </p> <p>SignalFlow has robust support for macOS and Linux (including Raspberry Pi), and has work-in-progress support for Windows. The overall project is currently in alpha status, and interfaces may change without warning.</p> <p>This documentation currently focuses specifically on Python interfaces and examples.</p>"},{"location":"#overview","title":"Overview","text":"<p>At its core, SignalFlow has a handful of key concepts.</p> <ul> <li>At the top level is the AudioGraph, which connects to the system's audio input/output hardware.</li> <li>The graph comprises of a network of Nodes, each of which performs a single function (for example, generating a cyclical waveform, or filtering an input node). Nodes are connected by input and output relationships: the output of one node may be used to control the frequency of another. As the output of the first node increases, the frequency of the second node increases correspondingly. This modulation is applied on a sample-by-sample basis: all modulation in SignalFlow happens at audio rate.</li> <li>Nodes may have multiple inputs, which determine which synthesis properties can be modulated at runtime.</li> <li>A node can also have Buffer properties, which contain audio waveform data that can be read and written to, for playback or recording of samples. </li> <li>Nodes can be grouped in a Patch, which is a user-defined configuration of nodes. A patch may have one or more named inputs that are defined by the user when creating the patch. Patches can be thought of like voices of a synthesizer. A patch can also be set to automatically remove itself from the graph when a specified node's playback is complete, which is important for automatic memory management.</li> </ul>"},{"location":"#example","title":"Example","text":"<p>Let's take a look at a minimal SignalFlow example. Here, we create and immediately start the <code>AudioGraph</code>, construct a stereo sine oscillator, connect the oscillator to the graph's output, and run the graph indefinitely.</p> <pre><code>from signalflow import *\n\ngraph = AudioGraph()\nsine = SineOscillator([440, 880])\nenvelope = ASREnvelope(0.1, 0.1, 0.5)\noutput = sine * envelope\noutput.play()\ngraph.wait()\n</code></pre> <p>This demo shows a few syntactical benefits that SignalFlow provides to make it easy to work with audio:</p> <ul> <li>The 2-item array of frequency values passed to <code>SineOscillator</code> is expanded to create a stereo, 2-channel output. If you passed a 10-item array, the output would have 10 channels. (Read more: Multichannel nodes)</li> <li>Mathematical operators like <code>*</code> can be used to multiply, add, subtract or divide the output of nodes, and creates a new output Node that corresponds to the output of the operation. This example uses an envelope to modulate the amplitude of an oscillator. (Read more: Node operators)</li> <li>Even through the envelope is mono and the oscillator is stereo, SignalFlow does the right thing and upmixes the envelope's values to create a stereo output, so that the same envelope shape is applied to the L and R channels of the oscillator, before creating a stereo output. This is called \"automatic upmixing\", and is handy when working with multichannel graphs. (Read more: Automatic upmixing)</li> </ul> <p>In subsequent examples, we will skip the <code>import</code> line and assume you have already imported everything from the <code>signalflow</code> namespace.</p> <p>Info</p> <p>If you want to keep your namespaces better separated, you might want to do something like the below. <pre><code>import signalflow as sf\n\ngraph = sf.AudioGraph()\nsine = sf.SineOscillator(440)\n...\n</code></pre></p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation</li> <li>Example code</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>For various code examples using SignalFlow, see <code>examples</code> in GitHub:</p> <p>https://github.com/ideoforms/signalflow/tree/master/examples</p>"},{"location":"getting-started/","title":"Getting started","text":""},{"location":"getting-started/#requirements","title":"Requirements","text":"<p>SignalFlow supports macOS, Linux (including Raspberry Pi), and has alpha support for Windows. </p>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#macos","title":"macOS","text":"<p>If you are an existing Python user and confident with the command line:</p> <p>macOS: Install SignalFlow from the command line</p> <p>If you're new to Python or getting started from scratch: </p> <p>macOS: Install SignalFlow with Visual Studio Code </p>"},{"location":"getting-started/#examples","title":"Examples","text":"<p>Several example scripts are included within the repo, covering simple control and modulation, FM synthesis, sample granulation, MIDI control, chaotic functions, etc.</p>"},{"location":"license/","title":"License","text":"<p>SignalFlow is under the MIT license.</p> <p>This means that you are welcome to use it for any purpose, including commercial usage, but must include the copyright notice above in any copies or derivative works.</p> <p>Please do let me know what you use it for! </p>"},{"location":"buffer/","title":"Buffer","text":"<p>Warning</p> <p>This documentation is a work-in-progress and may have sections that are missing or incomplete.</p> <p>A <code>Buffer</code> is an allocated area of memory that can be used to store single-channel or multi-channel data, which may represent an audio waveform or any other type of signal. </p> <ul> <li>A Buffer can be created from a sound file, an array of samples, or with an empty contents</li> <li>A Buffer can be passed to a Node or Patch as an input</li> <li>A Buffer can be exported to a sound file</li> <li>A Buffer's data can be directly accessed in memory as a numpy array, or by get/set methods</li> <li>The contents of a buffer can be combined with arithmetic operators</li> <li>Properties</li> <li>Buffer interpolation modes</li> <li>2D buffers</li> <li>Buffer applications: Sample recording and playback, control recording and playback, envelopes, waveshapers </li> <li>The total Buffer memory usage can be queried </li> </ul>"},{"location":"graph/","title":"The AudioGraph","text":"<p><code>AudioGraph</code> is the global audio processing system that schedules and performs audio processing. It is comprised of an interconnected network of Node and Patch objects, which audio flows through.</p> <p>Each time a new block of audio is requested by the system audio I/O layer, the <code>AudioGraph</code> object is responsible for traversing the tree of nodes and generating new samples by calling each <code>Node</code>'s <code>process</code> method.</p> <p>Why 'Graph'?</p> <p>You may be more familiar with \"graph\" being used to mean a data visualisation. In signal processing and discrete mathematics, the term \"graph\" is also used to denote a system of nodes (\"vertices\") related by connections (\"edges\"). Read more: Graph Theory Basics (Lumen Learning). </p> <p>\u2192 Next: Creating the graph</p>"},{"location":"graph/config/","title":"The AudioGraph","text":""},{"location":"graph/config/#graph-configuration","title":"Graph configuration","text":"<p>There are a number of graph configuration parameters that can be used to change the global behaviour of the audio system. This can be done programmatically, via a config file, or via environmental variables.</p> Parameter Description output_backend_name The name of the audio output backend to use, which can be one of: <code>jack</code>, <code>alsa</code>, <code>pulseaudio</code>, <code>coreaudio</code>, <code>wasapi</code>, <code>dummy</code>. Defaults to the first of these found on the system. Typically only required for Linux. output_device_name The name of the audio output device to use. This must precisely match the device's name in your system. If not found, <code>DeviceNotFoundException</code> is thrown when instantiating the graph. output_buffer_size The size of the hardware output audio buffer, in samples. A larger buffer reduces the chance of buffer overflows and glitches, but at the cost of higher latency. Note that this config option merely specifies the preferred output buffer size, which may not be available in the system hardware. To check the actual buffer size used by the AudioGraph, query <code>graph.output_buffer_size</code> after instantiation. input_device_name The name of the input device to use. input_buffer_size The size of the hardware input audio buffer. sample_rate The audio sample rate to use. cpu_usage_limit Imposes a hard limit on the CPU usage permitted by SignalFlow. If the estimated (single-core) CPU usage exceeds this value, no more nodes or patches can be created until it returns to below the limit. Floating-point value between 0..1, where 0.5 means 50% CPU."},{"location":"graph/config/#configuring-the-graph-programmatically","title":"Configuring the graph programmatically","text":"<p>To specify an alternative config, create and populate an <code>AudioGraphConfig</code> object before the graph is started:</p> <pre><code>config = AudioGraphConfig()\nconfig.output_device_name = \"MacBook Pro Speakers\"\nconfig.sample_rate = 44100\nconfig.output_buffer_size = 2048\n\ngraph = AudioGraph(config)\n</code></pre>"},{"location":"graph/config/#configuring-the-graph-via-signalflowconfig","title":"Configuring the graph via ~/.signalflow/config","text":"<p>To specify a configuration that is used by all future SignalFlow sessions, create a file <code>~/.signalflow/config</code>, containing one or more of the \"Graph configuration\" fields listed above.</p> <p>For example:</p> <pre><code>[audio]\nsample_rate = 48000\noutput_buffer_size = 256\ninput_buffer_size = 256\noutput_device_name = \"MacBook Pro Speakers\"\ninput_device_name = \"MacBook Pro Microphone\"\n</code></pre> <p>All fields are optional.</p> <p>A quick and easy way to edit your config, or create a new config file, is by using the <code>signalflow</code> command-line utility:</p> <pre><code>signalflow configure\n</code></pre> <p>This will use your default <code>$EDITOR</code> to open the configuration, or <code>pico</code> if no editor is specified.</p>"},{"location":"graph/config/#configuring-the-graph-via-environmental-variables","title":"Configuring the graph via environmental variables","text":"<p>SignalFlow config can also be set by setting an environmental variable in your shell. Variable names are identical to the upper-case version of the config string, prefixed with <code>SIGNALFLOW_</code>. For example:</p> <pre><code>export SIGNALFLOW_OUTPUT_DEVICE_NAME=\"MacBook Pro Speakers\"\nexport SIGNALFLOW_OUTPUT_BUFFER_SIZE=1024\n</code></pre>"},{"location":"graph/config/#printing-the-current-config","title":"Printing the current config","text":"<p>To print the current configuration to stdout:</p> <pre><code>graph.config.print()\n</code></pre> <p>\u2192 Next: Graph status and properties</p>"},{"location":"graph/creating/","title":"The AudioGraph","text":""},{"location":"graph/creating/#creating-the-graph","title":"Creating the graph","text":"<p>Creating the graph is simple: <code>graph = AudioGraph()</code></p> <p>By default, a new <code>AudioGraph</code> immediately connects to the system's default audio hardware device (via the integrated <code>libsoundio</code> library), using the system's default sample rate and buffer size.</p> <p>Info</p> <p>Note that the AudioGraph is a singleton object: only one AudioGraph can be created, which is shared globally.</p> <p>To prevent the graph from starting instantly (for example, if you want to use the graph in offline mode), pass <code>start=False</code> to the constructor.</p> <p>To configure graph playback or recording parameters, see AudioGraph: Configuration.</p> <p>\u2192 Next: Graph configuration</p>"},{"location":"graph/properties/","title":"The AudioGraph","text":""},{"location":"graph/properties/#status-and-properties","title":"Status and properties","text":"<p>A number of methods are provided to query the graph's current status and properties.</p>"},{"location":"graph/properties/#status","title":"Status","text":"<p>Querying <code>graph.status</code> returns a one-line description of the number of nodes and patches in the graph, and the estimated CPU and RAM usage:</p> <pre><code>&gt;&gt;&gt; graph.status\nAudioGraph: 235 active nodes, 6 patches, 13.95% CPU usage, 34.91MB memory usage\n</code></pre> <p>To automatically poll and print the graph's status periodically, call <code>graph.poll(interval)</code>, where <code>interval</code> is in seconds:</p> <pre><code>&gt;&gt;&gt; graph.poll(1)\nAudioGraph: 118 active nodes, 3 patches, 7.09% CPU usage, 34.91MB memory usage\nAudioGraph: 118 active nodes, 3 patches, 7.16% CPU usage, 34.91MB memory usage\nAudioGraph: 40 active nodes, 1 patch, 2.60% CPU usage, 34.91MB memory usage\n</code></pre> <p>To stop polling, call <code>graph.poll(0)</code>.</p>"},{"location":"graph/properties/#structure","title":"Structure","text":"<p>Querying <code>graph.structure</code> returns a multi-line string describing every Node in the graph, their parameter values, and their connectivity structure.</p> <pre><code>&gt;&gt;&gt; graph.structure\n * audioout-soundio\n   input0:\n    * linear-panner\n      pan: 0.000000\n      input:\n       * multiply\n         input1: 0.251189\n         input0:\n          * sine\n            frequency: 440.000000\n</code></pre>"},{"location":"graph/properties/#other-graph-properties","title":"Other graph properties","text":"<ul> <li><code>graph.node_count</code> (int): Returns the current number of Nodes in the graph (including within patches)</li> <li><code>graph.patch_count</code> (int): Returns the current number of Patches in the graph</li> <li><code>cpu_usage</code> (float): Returns the current CPU usage, between 0.0 (0%) and 1.0 (100%). CPU usage can be lowered by increasing the output buffer size.</li> <li><code>memory_usage</code> (int): Returns the current RAM usage, in bytes. This is typically mostly used by waveform data in Buffers.</li> <li><code>num_output_channels</code> (int): Returns the graph's current output channel count, which is typically identical to the number of channels supported by the audio output device.</li> <li><code>output_buffer_size</code> (int): Returns the current hardware output buffer size, in bytes.</li> </ul> <p>\u2192 Next: Recording graph output</p>"},{"location":"graph/recording/","title":"The AudioGraph","text":""},{"location":"graph/recording/#recording-the-audio-output-of-the-graph","title":"Recording the audio output of the graph","text":"<p>Convenience methods are provided to make it easy to record the global audio output when rendering audio in real-time:</p> <pre><code>graph.start_recording(\"filename.wav\")\n...\ngraph.stop_recording()\n</code></pre> <p>To record output in formats other than the default stereo, <code>start_recording</code> takes a <code>num_channels</code> argument that can be used to specify an alternative channel count.</p> <p>Note</p> <p>At present, only .wav is supported as an output format for global audio recordings. </p>"},{"location":"graph/recording/#offline-non-real-time-rendering","title":"Offline (non-real-time) rendering","text":"<p>It is also possible to perform non-real-time rendering of a synthesis graph,  by synthesizing audio output to a <code>Buffer</code> which can then be saved to disk:   </p> <pre><code># Create an AudioGraph with a dummy output device\ngraph = AudioGraph(output_device=AudioOut_Dummy(2))\n\n# Create a buffer that will be used to store the audio output\nbuffer = Buffer(2, graph.sample_rate * 4)\n\n# Create a synthesis graph to render\nfreq = SawLFO(1, 200, 400)\nsine = SineOscillator([freq, freq+10])\ngraph.play(sine)\n\n# Render to the buffer. Non-real-time, so happens instantaneously.\n# Note that the graph renders as many samples as needed to fill the buffer.\ngraph.render_to_buffer(buffer)\n\n# Write the buffer contents to a file\nbuffer.save(\"output.wav\")\n\n# Finally, tear down the buffer\ngraph.destroy()\n</code></pre> <p>\u2192 Next: Clearing and stopping the graph</p>"},{"location":"graph/stopping/","title":"The AudioGraph","text":""},{"location":"graph/stopping/#clearing-and-stopping-the-graph","title":"Clearing and stopping the graph","text":"<p>To clear all nodes and patches from the graph but leave it running for further audio synthesis:</p> <pre><code>&gt;&gt;&gt; graph.clear()\n</code></pre> <p>To stop the graph and pause audio I/O:</p> <pre><code>&gt;&gt;&gt; graph.stop()\n</code></pre> <p>To permanently destroy the graph:</p> <pre><code>&gt;&gt;&gt; graph.destroy()\n</code></pre>"},{"location":"howto/","title":"Howto","text":"<p>Warning</p> <p>This documentation is a work-in-progress and may have sections that are missing or incomplete.</p> <p>Tutorials on common tasks with SignalFlow.</p>"},{"location":"howto/midi/","title":"Howto: MIDI control","text":""},{"location":"installation/linux/","title":"Getting started","text":""},{"location":"installation/linux/#requirements","title":"Requirements","text":"<p>SignalFlow supports macOS, Linux (including Raspberry Pi), and has alpha support for Windows. </p> <p>Python 3.8 or above is required. On macOS, we recommend installing an up-to-date version of Python3 using Homebrew: <code>brew install python3</code>.</p>"},{"location":"installation/linux/#installation","title":"Installation","text":"<p>On macOS and Linux x86_64, SignalFlow can be installed using <code>pip</code>:</p> <pre><code>pip3 install signalflow \n</code></pre> <p>Verify that the installation has worked correctly by using the <code>signalflow</code> command-line tool to play a test tone through your default system audio output:</p> <pre><code>signalflow test\n</code></pre> <p>For more detailed installation information, including Windows install and compilation from source, see the README.</p>"},{"location":"installation/linux/#examples","title":"Examples","text":"<p>Several example scripts are included within the repo, covering simple control and modulation, FM synthesis, sample granulation, MIDI control, chaotic functions, etc.</p>"},{"location":"installation/macos/command-line/","title":"SignalFlow: Command-line installation for macOS","text":"<p>These instructions assume you have a working version of Python 3.8+, installed either via Homebrew or from Python.org.</p>"},{"location":"installation/macos/command-line/#1-set-up-a-virtual-environment","title":"1. Set up a virtual environment","text":"<p>We strongly recommend setting up a dedicated Python \"virtual environment\" for SignalFlow</p> <pre><code>python3 -m venv signalflow-env\nsource signalflow-env/bin/activate\n</code></pre>"},{"location":"installation/macos/command-line/#2-install-signalflow","title":"2. Install SignalFlow","text":"<p>Installing SignalFlow with <code>pip</code>:</p> <pre><code>pip3 install signalflow jupyter\npython3 -m ipykernel install --name signalflow-env\n</code></pre> <p>If the installation succeeds, you should see <code>Successfully installed signalflow</code>.</p>"},{"location":"installation/macos/command-line/#3-line-test","title":"3. Line test","text":"<p>The installation of SignalFlow includes a command-line tool, <code>signalflow</code>, that can be used to test and configure the framework. Check that the installation has succeeded by playing a test tone through your default system audio output.</p> <p>This may take a few seconds to run for the first time. To exit the test, press ctrl-C (<code>^C</code>).</p> <pre><code>signalflow test\n</code></pre>"},{"location":"installation/macos/command-line/#examples","title":"Examples","text":"<p>Several example scripts are included within the repo, covering simple control and modulation, FM synthesis, sample granulation, MIDI control, chaotic functions, etc.</p>"},{"location":"installation/macos/easy/","title":"SignalFlow: Easy install for macOS","text":"<p>The simplest way to start exploring SignalFlow is with the free Visual Studio Code editor. Visual Studio Code can edit interactive \"Jupyter\" notebooks, which allow you to run and modify blocks of Python code in real-time, which is a great way to experiment live with audio synthesis.</p> <p>You'll only need to do this installation process once. Once setup, experimenting with SignalFlow is as simple as opening Visual Studio Code.  </p>"},{"location":"installation/macos/easy/#1-install-python","title":"1. Install Python","text":"<p>Download and install the latest version of Python (currently 3.12).</p> <p>Download Python</p>"},{"location":"installation/macos/easy/#2-download-and-install-visual-studio-code","title":"2. Download and install Visual Studio Code","text":"<p>Download and install the latest version of Visual Studio Code.</p> <p>Download Visual Studio Code</p> <p>Once installed, open <code>Applications</code> and run <code>Visual Studio Code</code>.</p>"},{"location":"installation/macos/easy/#3-create-a-new-visual-studio-code-workspace","title":"3. Create a new Visual Studio Code workspace","text":"<p>In Visual Studio code, create a new folder to contain your new SignalFlow project:</p> <ul> <li>Select <code>File \u2192 Open Folder...</code></li> <li>Select <code>New Folder</code>, and pick a name for your new project folder</li> </ul> <p>Where to put your workspace</p> <p>You can store your project workspace anywhere on your drive. The workspace can hold multiple notebooks, audio files, etc.</p> <p>Trusted workspaces</p> <p>If Visual Studio asks \"Do you trust the authors of the files in this folder?\", select \"Yes, I trust the authors\". This is a security mechanism to protect you against untrusted third-party code.</p>"},{"location":"installation/macos/easy/#4-install-the-python-and-jupyter-extensions","title":"4. Install the Python and Jupyter extensions","text":"<p>Visual Studio Code requires extensions to be installed to handle Python and Jupyter files.</p> <p>In Visual Studio Code, select the <code>Extensions</code> icon from in the far-left column (or press <code>\u21e7\u2318X</code>), and install the <code>Python</code> and <code>Jupyter</code> extensions by searching for their names and clicking \"Install\" on each.</p> <p>Once installation has finished, close the <code>Extensions</code> tab.</p>"},{"location":"installation/macos/easy/#5-create-a-notebook-in-visual-studio-code","title":"5. Create a notebook in Visual Studio Code","text":"<p>Select <code>File \u2192 New File...</code> (<code>^\u2325\u2318N</code>), and select <code>Jupyter Notebook</code>. You should see the screen layout change to display an empty black text block (in Jupyter parlance, a \"cell\"). </p>"},{"location":"installation/macos/easy/#6-create-a-python-virtual-environment-to-use","title":"6. Create a Python virtual environment to use","text":"<p>Click the button marked <code>Select Kernel</code> in the top right. </p> <ul> <li>Select <code>Python Environments...</code></li> <li>Select <code>Create Python Environment</code></li> <li>Select <code>Venv</code></li> <li>Finally, select the version of Python you just installed (<code>3.12.x</code>).</li> </ul> <p>Visual Studio Code will launch into some activity, in which it is installing necessary libraries and creating a Python \"virtual environment\", which is an isolated area of the filesystem containing all of the packages needed for this working space. Working in different virtual environments for different projects is good practice to minimise the likelihood of conflicts and disruptions.</p> <p>When the setup is complete, the button in the top right should change to say <code>.venv (Python 3.12.x)</code>.</p> <p>You're now all set to start writing code!</p>"},{"location":"installation/macos/easy/#7-start-writing-code","title":"7. Start writing code","text":"<p>In the first block, type:</p> <pre><code>print(\"Hello, world\")\n</code></pre> <p>To run the cell, press <code>^\u21b5</code> (control-enter). You should see \"Hello, world\" appear below the cell. You're now able to edit, change and run Python code in real-time!</p> <p>Keyboard shortcuts</p> <ul> <li>Navigate between cells with the arrow keys</li> <li>Use <code>enter</code> to begin editing a cell, and <code>escape</code> to end editing and move to select mode</li> <li>In select mode, use <code>b</code> to add a cell after the current cell, and <code>a</code> to add a cell before it </li> <li>To evaluate a cell and move on to the next cell, use <code>\u21e7\u21b5</code> (shift-enter)</li> </ul>"},{"location":"installation/macos/easy/#8-start-a-signalflow-session","title":"8. Start a SignalFlow session","text":"<p>Clear the first cell, and replace it with:</p> <pre><code>from signalflow import *\n</code></pre> <p>Run the cell with <code>^\u21b5</code>. This imports all of the SignalFlow commands and classes.</p> <p>Create a new cell (<code>b</code>), and in the new cell, run:</p> <pre><code>graph = AudioGraph()\n</code></pre> <p>This will create and start a new global audio processing system, using the system's default audio output. You should see the name of the audio device printed to the notebook.</p> <p>In a new cell, run:</p> <pre><code>sine = SineOscillator(440) * 0.1\nsine.play()\n</code></pre> <p>This will create a sine oscillator, attenuate it, and play it from the system. Hopefully you should now hear a tone playing from your speaker or headphones.</p> <p>To stop the playback, create a new cell and run:</p> <pre><code>sine.stop()\n</code></pre>"},{"location":"node/","title":"Nodes","text":"<p>A <code>Node</code> object is an audio processing unit that performs one single function. For example, a Node's role may be to synthesize a waveform, read from a buffer, or take two input Nodes and sum their values.</p> <ul> <li>Nodes are played and stopped by connecting them to the AudioGraph </li> <li>A node has one or more audio-rate inputs, which can be modulated by other nodes</li> <li>Some nodes can be triggered with trigger inputs \u2014 for example, to restart playback, or set the position of an envelope</li> <li>Some nodes can be used to play back the contents of buffer inputs, or can use buffer data as a source of modulation</li> <li>The output of multiple nodes can be combined and modulated with use of the standard Python operators (<code>+</code>, <code>-</code>, <code>*</code>, <code>%</code>, etc)</li> <li>The output of a node can be mono (single-channel) or multichannel</li> <li>A Node's status and output can be examined by querying its properties</li> <li>Some Nodes generate unpredictable stochastic output, which can be controlled via its internal random number generator </li> <li>Details of how to create a new Node type are detailed in Developing a new Node class</li> <li>For an overview of every type of Node available in SignalFlow, see the Node Reference Library</li> </ul> <p>\u2192 Next: Node playback</p>"},{"location":"node/developing/","title":"Nodes","text":""},{"location":"node/developing/#developing-new-node-classes","title":"Developing new Node classes","text":"<p>See CONTRIBUTING.md</p>"},{"location":"node/inputs/","title":"Nodes","text":""},{"location":"node/inputs/#node-inputs","title":"Node inputs","text":"<p>A node has three different classes of input:</p> <ul> <li>Audio-rate inputs: Takes the output of another node as an input, for continuous modulation of synthesis parameters</li> <li>Trigger inputs: Used to trigger discrete control events \u2014 for example, restarting buffer playback</li> <li>Buffer inputs: Used to pass the contents of an audio buffer to a node \u2014 for example, as a source of audio samples, or an envelope shape</li> </ul>"},{"location":"node/inputs/#audio-rate-inputs","title":"Audio-rate inputs","text":"<p>Virtually every node has one or more audio-rate inputs. Put simply, an audio-rate input is the output of another node. Let's look at a short example:</p> <pre><code>lfo = SineLFO()\nsignal = SquareOscillator(frequency=200, width=lfo)\n</code></pre> <p>In this case, we are passing the output of a <code>SineLFO</code> as the pulse width of a <code>SquareOscillator</code>. This is an audio-rate input.</p> <p>Although it's not obvious, the <code>frequency</code> parameter is also an audio-rate input. Any constant value (such as the <code>200</code> here) is behind the scenes implemented as a <code>Constant</code> node, which continuously outputs the value at an audio rate.</p> <p>All audio-rate inputs can be modified just like a normal Python property. For example:</p> <pre><code>signal.frequency = TriangleOscillator(0.5, 100, 1000)\n</code></pre>"},{"location":"node/inputs/#variable-input-nodes","title":"Variable input nodes","text":"<p>Some nodes have a variable number of inputs, which can change over the Node's lifetime. For example, <code>Sum()</code> takes an arbitrary number of input Nodes, and generates an output which is the sum of all of its inputs.</p> <p>For variable-input nodes such as this, audio-rate inputs are added with <code>add_input()</code>, and can be removed with <code>remove_input()</code>.</p> <pre><code>a = Constant(1)\nb = Constant(2)\nc = Constant(3)\nsum = Sum()\nsum.add_input(a)\nsum.add_input(b)\nsum.add_input(c)\n# sum will now generate an output of 6.0\n</code></pre> <p>It is possible to check whether a Node object takes variable inputs by querying <code>node.has_variable_inputs</code>.</p>"},{"location":"node/inputs/#triggers","title":"Triggers","text":"<p>When working with sequencing and timing, it is often useful be able to trigger discrete events within a node. This is where trigger inputs come in handy.</p> <p>There are two different ways to handle trigger inputs:</p> <ul> <li>by calling the <code>trigger()</code> method on a <code>Node</code></li> <li>by passing a Node to an input that corresponds to an audio-rate trigger</li> </ul>"},{"location":"node/inputs/#calling-trigger","title":"Calling trigger()","text":"<p>To generate trigger events at arbitrary times, call <code>node.trigger()</code>. For example:</p> <pre><code>freq_env = Line(10000, 100, 0.5)\nsine = SineOscillator(freq_env)\nsine.play()\nwhile True:\n    freq_env.trigger()\n    graph.wait(1)\n</code></pre> <p>This is useful because it can be done outside the audio thread. For example, <code>trigger()</code> could be called each time a MIDI note event is received. </p> <p>The <code>trigger()</code> method takes an optional <code>name</code> parameter, which is used by <code>Node</code> classes containing more than one type of trigger. This example uses the <code>set_position</code> trigger of <code>BufferPlayer</code> to seek to a new location in the sample every second. </p> <pre><code>buffer = Buffer(\"../audio/stereo-count.wav\")\nplayer = BufferPlayer(buffer, loop=True)\nplayer.play()\nwhile True:\n    player.trigger(\"set_position\", random_uniform(0, buffer.duration))\n    graph.wait(1)\n</code></pre> <p>Note</p> <p>Because the <code>trigger</code> method happens outside the audio thread, it will take effect at the start of the next audio block. This means that, if you are running at 44.1kHz with an audio buffer size of 1024 samples, this could introduce a latency of up to <code>1024/44100 = 0.023s</code>.   For time-critical events like drum triggers, this can be minimised by reducing the hardware output buffer size.</p> <p>This constraint also means that only one event can be triggered per audio block. To trigger events at a faster rate than the hardware buffer size allows, see Audio-rate triggers below.  </p>"},{"location":"node/inputs/#audio-rate-triggers","title":"Audio-rate triggers","text":"<p>It is often desirable to trigger events using the audio-rate output of another Node object as a source of trigger events, to give sample-level precision in timing. Most nodes that support <code>trigger</code> inputs can also be triggered by a corresponding audio-rate input. </p> <p>Triggers happen at zero-crossings \u2014 that is, when the output of the node passes above zero (i.e., from <code>&lt;= 0</code> to <code>&gt;0</code>). For example, to create a clock with an oscillating tempo to re-trigger buffer playback: </p> <pre><code>clock = Impulse(SineLFO(0.2, 1, 10))\nbuffer = Buffer(\"../audio/stereo-count.wav\")\nplayer = BufferPlayer(buffer, loop=True, clock=clock)\nplayer.play()\n</code></pre> <p>This can be used to your advantage with the boolean operator nodes. </p> <pre><code>on_the_right = MouseX() &gt; 0.5\nenvelope = ASREnvelope(0, 0, 0.5, clock=on_the_right)\nsquare = SquareOscillator(100)\noutput = envelope * square * 0.1\noutput.play()\n</code></pre> <p>TODO: Should the name of the trigger() event always be identical to the trigger input name? So <code>clock</code> for envelopes, buffer player, etc...?</p>"},{"location":"node/inputs/#buffer-inputs","title":"Buffer inputs","text":"<p>The third type of input supported by nodes is the buffer. Nodes often take buffer inputs as sources of audio samples. They are also useful as sources of envelope shape data (for example, to shape the grains of a Granulator), or general control data (for example, recording motion patterns from a <code>MouseX</code> input).</p> <pre><code>buffer = Buffer(\"../audio/stereo-count.wav\")\nplayer = BufferPlayer(buffer, loop=True)\n</code></pre> <p>\u2192 Next: Operators</p>"},{"location":"node/library/","title":"Node reference library","text":""},{"location":"node/library/#analysis","title":"analysis","text":"<ul> <li>CrossCorrelate <code>(input=nullptr, buffer=nullptr, hop_size=0)</code></li> <li>OnsetDetector <code>(input=0.0, threshold=2.0, min_interval=0.1)</code></li> <li>VampAnalysis <code>(input=0.0, plugin_id=\"vamp-example-plugins:spectralcentroid:linearcentroid\")</code></li> </ul>"},{"location":"node/library/#buffer","title":"buffer","text":"<ul> <li>BeatCutter <code>(buffer=nullptr, segment_count=8, stutter_probability=0.0, stutter_count=1, jump_probability=0.0, duty_cycle=1.0, rate=1.0, segment_rate=1.0)</code></li> <li>BufferLooper <code>(buffer=nullptr, input=0.0, feedback=0.0, loop_playback=false, loop_record=false)</code></li> <li>BufferPlayer <code>(buffer=nullptr, rate=1.0, loop=0, start_time=nullptr, end_time=nullptr, clock=nullptr)</code></li> <li>BufferRecorder <code>(buffer=nullptr, input=0.0, feedback=0.0, loop=false)</code></li> <li>FeedbackBufferReader <code>(buffer=nullptr)</code></li> <li>FeedbackBufferWriter <code>(buffer=nullptr, input=0.0, delay_time=0.1)</code></li> <li>GrainSegments <code>(buffer=nullptr, clock=0, target=0, offsets={}, values={}, durations={})</code></li> <li>Granulator <code>(buffer=nullptr, clock=0, pos=0, duration=0.1, pan=0.0, rate=1.0, max_grains=2048)</code></li> <li>SegmentPlayer <code>(buffer=nullptr, onsets={})</code></li> </ul>"},{"location":"node/library/#control","title":"control","text":"<ul> <li>MouseX <code>()</code></li> <li>MouseY <code>()</code></li> <li>MouseDown <code>(button_index=0)</code></li> </ul>"},{"location":"node/library/#envelope","title":"envelope","text":"<ul> <li>ADSREnvelope <code>(attack=0.1, decay=0.1, sustain=0.5, release=0.1, gate=0)</code></li> <li>ASREnvelope <code>(attack=0.1, sustain=0.5, release=0.1, curve=1.0, clock=nullptr)</code></li> <li>DetectSilence <code>(input=nullptr, threshold=0.00001)</code></li> <li>Envelope <code>(levels=std::vector&lt;NodeRef&gt; ( ), times=std::vector&lt;NodeRef&gt; ( ), curves=std::vector&lt;NodeRef&gt; ( ), clock=nullptr, loop=false)</code></li> <li>Line <code>(from=0.0, to=1.0, time=1.0, loop=0, clock=nullptr)</code></li> <li>RectangularEnvelope <code>(sustain=1.0, clock=nullptr)</code></li> </ul>"},{"location":"node/library/#fft","title":"fft","text":"<ul> <li>FFTContinuousPhaseVocoder <code>(input=nullptr, rate=1.0)</code></li> <li>FFTConvolve <code>(input=nullptr, buffer=nullptr)</code></li> <li>FFT <code>(input=0.0, fft_size=SIGNALFLOW_DEFAULT_FFT_SIZE, hop_size=SIGNALFLOW_DEFAULT_FFT_HOP_SIZE, window_size=0, do_window=true)</code></li> <li>FFTNode <code>(fft_size=None, hop_size=None, window_size=None, do_window=None)</code></li> <li>FFTOpNode <code>(input=nullptr)</code></li> <li>FFTFindPeaks <code>(input=0, prominence=1, threshold=0.000001, count=SIGNALFLOW_MAX_CHANNELS, interpolate=true)</code></li> <li>IFFT <code>(input=nullptr, do_window=false)</code></li> <li>FFTLPF <code>(input=0, frequency=2000)</code></li> <li>FFTNoiseGate <code>(input=0, threshold=0.5)</code></li> <li>FFTPhaseVocoder <code>(input=nullptr)</code></li> <li>FFTTonality <code>(input=0, level=0.5, smoothing=0.9)</code></li> <li>FFTZeroPhase <code>(input=0)</code></li> </ul>"},{"location":"node/library/#operators","title":"operators","text":"<ul> <li>Add <code>(a=0, b=0)</code></li> <li>AmplitudeToDecibels <code>(a=0)</code></li> <li>DecibelsToAmplitude <code>(a=0)</code></li> <li>ChannelArray <code>()</code></li> <li>ChannelMixer <code>(num_channels=1, input=0, amplitude_compensation=true)</code></li> <li>ChannelSelect <code>(input=nullptr, offset=0, maximum=0, step=1)</code></li> <li>Equal <code>(a=0, b=0)</code></li> <li>NotEqual <code>(a=0, b=0)</code></li> <li>GreaterThan <code>(a=0, b=0)</code></li> <li>GreaterThanOrEqual <code>(a=0, b=0)</code></li> <li>LessThan <code>(a=0, b=0)</code></li> <li>LessThanOrEqual <code>(a=0, b=0)</code></li> <li>Modulo <code>(a=0, b=0)</code></li> <li>Abs <code>(a=0)</code></li> <li>If <code>(a=0, value_if_true=0, value_if_false=0)</code></li> <li>Divide <code>(a=1, b=1)</code></li> <li>FrequencyToMidiNote <code>(a=0)</code></li> <li>MidiNoteToFrequency <code>(a=0)</code></li> <li>Multiply <code>(a=1.0, b=1.0)</code></li> <li>Pow <code>(a=0, b=0)</code></li> <li>RoundToScale <code>(a=0)</code></li> <li>Round <code>(a=0)</code></li> <li>ScaleLinExp <code>(input=0, a=0, b=1, c=1, d=10)</code></li> <li>ScaleLinLin <code>(input=0, a=0, b=1, c=1, d=10)</code></li> <li>Subtract <code>(a=0, b=0)</code></li> <li>Sum <code>()</code></li> <li>Sin <code>(a=0)</code></li> <li>Cos <code>(a=0)</code></li> <li>Tan <code>(a=0)</code></li> <li>Tanh <code>(a=0)</code></li> </ul>"},{"location":"node/library/#oscillators","title":"oscillators","text":"<ul> <li>Constant <code>(value=0)</code></li> <li>Impulse <code>(frequency=1.0)</code></li> <li>LFO <code>(frequency=1.0, min=0.0, max=1.0, phase=0.0)</code></li> <li>SawLFO <code>(frequency=1.0, min=0.0, max=1.0, phase=0.0)</code></li> <li>SawOscillator <code>(frequency=440, phase=nullptr)</code></li> <li>SineLFO <code>(frequency=1.0, min=0.0, max=1.0, phase=0.0)</code></li> <li>SineOscillator <code>(frequency=440)</code></li> <li>SquareLFO <code>(frequency=1.0, min=0.0, max=1.0, width=0.5, phase=0.0)</code></li> <li>SquareOscillator <code>(frequency=440, width=0.5)</code></li> <li>TriangleLFO <code>(frequency=1.0, min=0.0, max=1.0, phase=0.0)</code></li> <li>TriangleOscillator <code>(frequency=440)</code></li> <li>Wavetable <code>(buffer=nullptr, frequency=440, phase=0, sync=0, phase_map=nullptr)</code></li> <li>Wavetable2D <code>(buffer=nullptr, frequency=440, crossfade=0.0, phase=0.0, sync=0)</code></li> </ul>"},{"location":"node/library/#processors","title":"processors","text":"<ul> <li>Clip <code>(input=nullptr, min=-1.0, max=1.0)</code></li> <li>Fold <code>(input=nullptr, min=-1.0, max=1.0)</code></li> <li>Smooth <code>(input=nullptr, smooth=0.99)</code></li> <li>WetDry <code>(dry_input=nullptr, wet_input=nullptr, wetness=0.0)</code></li> <li>Wrap <code>(input=nullptr, min=-1.0, max=1.0)</code></li> </ul>"},{"location":"node/library/#processorsdelays","title":"processors/delays","text":"<ul> <li>AllpassDelay <code>(input=0.0, delay_time=0.1, feedback=0.5, max_delay_time=0.5)</code></li> <li>CombDelay <code>(input=0.0, delay_time=0.1, feedback=0.5, max_delay_time=0.5)</code></li> <li>OneTapDelay <code>(input=0.0, delay_time=0.1, max_delay_time=0.5)</code></li> <li>Stutter <code>(input=0.0, stutter_time=0.1, stutter_count=1, clock=nullptr, max_stutter_time=1.0)</code></li> </ul>"},{"location":"node/library/#processorsdistortion","title":"processors/distortion","text":"<ul> <li>Resample <code>(input=0, sample_rate=44100, bit_rate=16)</code></li> <li>SampleAndHold <code>(input=nullptr, clock=nullptr)</code></li> <li>Squiz <code>(input=0.0, rate=2.0, chunk_size=1)</code></li> <li>WaveShaper <code>(input=0.0, buffer=nullptr)</code></li> </ul>"},{"location":"node/library/#processorsdynamics","title":"processors/dynamics","text":"<ul> <li>Compressor <code>(input=0.0, threshold=0.1, ratio=2, attack_time=0.01, release_time=0.1, sidechain=nullptr)</code></li> <li>Gate <code>(input=0.0, threshold=0.1)</code></li> <li>Maximiser <code>(input=0.0, ceiling=0.5, attack_time=1.0, release_time=1.0)</code></li> <li>RMS <code>(input=0.0)</code></li> </ul>"},{"location":"node/library/#processorsfilters","title":"processors/filters","text":"<ul> <li>BiquadFilter <code>(input=0.0, filter_type=SIGNALFLOW_FILTER_TYPE_LOW_PASS, cutoff=440, resonance=0.0, peak_gain=0.0)</code></li> <li>EQ <code>(input=0.0, low_gain=1.0, mid_gain=1.0, high_gain=1.0, low_freq=500, high_freq=5000)</code></li> <li>MoogVCF <code>(input=0.0, cutoff=200.0, resonance=0.0)</code></li> <li>SVFilter <code>(input=0.0, filter_type=SIGNALFLOW_FILTER_TYPE_LOW_PASS, cutoff=440, resonance=0.0)</code></li> </ul>"},{"location":"node/library/#processorspanning","title":"processors/panning","text":"<ul> <li>AzimuthPanner <code>(num_channels=2, input=0, pan=0.0, width=1.0)</code></li> <li>ChannelPanner <code>(num_channels=2, input=0, pan=0.0, width=1.0)</code></li> <li>SpatialPanner <code>(env=nullptr, input=0.0, x=0.0, y=0.0, z=0.0, radius=1.0, algorithm=\"dbap\")</code></li> <li>StereoBalance <code>(input=0, balance=0)</code></li> <li>StereoPanner <code>(input=0, pan=0.0)</code></li> <li>StereoWidth <code>(input=0, width=1)</code></li> </ul>"},{"location":"node/library/#sequencing","title":"sequencing","text":"<ul> <li>ClockDivider <code>(clock=0, factor=1)</code></li> <li>Counter <code>(clock=0, min=0, max=2147483647)</code></li> <li>Euclidean <code>(clock=0, sequence_length=0, num_events=0)</code></li> <li>FlipFlop <code>(clock=0)</code></li> <li>ImpulseSequence <code>(sequence=std::vector&lt;int&gt; ( ), clock=nullptr)</code></li> <li>Index <code>(list={}, index=0)</code></li> <li>Latch <code>(set=0, reset=0)</code></li> <li>Sequence <code>(sequence=std::vector&lt;float&gt; ( ), clock=nullptr)</code></li> </ul>"},{"location":"node/library/#stochastic","title":"stochastic","text":"<ul> <li>Logistic <code>(chaos=3.7, frequency=0.0)</code></li> <li>PinkNoise <code>(low_cutoff=20.0, high_cutoff=20000.0, reset=nullptr)</code></li> <li>RandomBrownian <code>(min=-1.0, max=1.0, delta=0.01, clock=nullptr, reset=nullptr)</code></li> <li>RandomChoice <code>(values=std::vector&lt;float&gt; ( ), clock=nullptr, reset=nullptr)</code></li> <li>RandomCoin <code>(probability=0.5, clock=nullptr, reset=nullptr)</code></li> <li>RandomExponentialDist <code>(scale=0.0, clock=nullptr, reset=nullptr)</code></li> <li>RandomExponential <code>(min=0.001, max=1.0, clock=nullptr, reset=nullptr)</code></li> <li>RandomGaussian <code>(mean=0.0, sigma=0.0, clock=nullptr, reset=nullptr)</code></li> <li>RandomImpulseSequence <code>(probability=0.5, length=8, clock=nullptr, explore=nullptr, generate=nullptr, reset=nullptr)</code></li> <li>RandomImpulse <code>(frequency=1.0, distribution=SIGNALFLOW_EVENT_DISTRIBUTION_UNIFORM, reset=nullptr)</code></li> <li>RandomUniform <code>(min=0.0, max=1.0, clock=nullptr, reset=nullptr)</code></li> <li>StochasticNode <code>(reset=nullptr)</code></li> <li>WhiteNoise <code>(frequency=0.0, min=-1.0, max=1.0, interpolate=true, random_interval=true, reset=nullptr)</code></li> </ul>"},{"location":"node/multichannel/","title":"Nodes","text":""},{"location":"node/multichannel/#multichannel-nodes","title":"Multichannel nodes","text":"<p>When passing a value to audio-rate input of a Node, the signal is by default monophonic (single-channel). For example, <code>SquareOscillator(440)</code> generates a 1-channel output.</p> <p>It is possible to generate multi-channel output by passing an array of values in the place of a constant. For example, <code>SquareOscillator([440, 880])</code> generates stereo output with a different frequency in the L and R channels. </p> <p>There is no limit to the number of channels that can be generated by a node. For example, <code>SquareOscillator(list(100 + 50 * n for n in range(100)))</code> will create a node with 100-channel output, each with its own frequency. </p> <pre><code>&gt;&gt;&gt; sq = SquareOscillator([100 + 50 * n for n in range(100)])\n&gt;&gt;&gt; print(sq.num_output_channels)\n100\n</code></pre>"},{"location":"node/multichannel/#automatic-upmixing","title":"Automatic upmixing","text":"<p>There are generally multiple inputs connected to a node, which may themselves have differing number of channels. For example, <code>SquareOscillator(frequency=[100, 200, 300, 400, 500], width=0.7)</code> has a 5-channel input and a 1-channel input. In cases like this, the output of the nodes with fewer channels is upmixed to match the higher-channel inputs.</p> <p>Upmixing here means simply duplicating the output until it reaches the desired number of channels. In the above case, the <code>width</code> input will be upmixed to generate 5 channels, all containing <code>0.7</code>.</p> <p>If <code>width</code> were a stereo input with L and R channels, the output would be tiled, alternating between the channels. Each frame of stereo input would then be upmixed to contain <code>[L, R, L, R, L]</code>, where <code>L</code> and <code>R</code> are the samples corresponding to the L and R channels.</p> <p>The key rule is that, for nodes that support upmixing, the output signal has as many channels as the input signal with the highest channel count. </p> <p>This process percolates through the signal chain. For example:</p> <pre><code>SquareOscillator(frequency=SineLFO([1, 3, 5], min=440, max=880),\n                 width=SawLFO([0.5, 0.6], min=0.25, max=0.75))\n</code></pre> <ul> <li>The <code>min</code> and <code>max</code> inputs of the <code>frequency</code> LFO would be upmixed to 3 channels each</li> <li>The <code>min</code> and <code>max</code> inputs of the <code>width</code> LFO would be upmixed to 2 channels each</li> <li>Then, the output of the <code>width</code> node would be upmixed from 2 to 3 channels</li> </ul>"},{"location":"node/multichannel/#nodes-with-fixed-inputoutput-channels","title":"Nodes with fixed input/output channels","text":"<p>Some nodes have immutable numbers of input/output channels. For example:</p> <ul> <li><code>StereoPanner</code> has 1 input channel and 2 output channels</li> <li><code>StereoBalance</code> has 2 input channels and 2 output channels</li> <li><code>ChannelMixer</code> has an arbitrary number of input channels, but a fixed, user-specified number of output channels</li> </ul> <p>Even Nodes that do not have an obvious input (e.g. <code>BufferPlayer</code>) have input channels, for modulation inputs (for example, modulating the rate of the buffer).</p> <p>When two nodes are connected together with incompatible channel counts (for example, connecting a <code>StereoBalance</code> into a <code>StereoMixer</code>), an <code>InvalidChannelCountException</code> will be raised.</p>"},{"location":"node/multichannel/#the-channel-node-classes","title":"The Channel* node classes","text":"<p>There are a number of Node subclasses dedicated to channel handling.</p> <ul> <li>ChannelArray: Concatenates the channels of multiple nodes, so that calling <code>ChannelMix</code> with nodes of <code>N</code> and <code>M</code> channels will produce an output of <code>N + M</code> channels.</li> <li>ChannelMixer: Reduces or expands the number of channels by evenly spreading the audio across the output channels.</li> <li>ChannelSelect: Selects sub-channels of the input, either individually or by group. </li> </ul>"},{"location":"node/multichannel/#querying-channel-subsets-with-the-index-operator","title":"Querying channel subsets with the index operator","text":"<p>Single channels of a multi-channel node can be accessed using the index <code>[]</code> operator. For example:</p> <pre><code>square = SquareOscillator([440, 441, 442, 443])\noutput = square[0]\n# output now contains a mono output, with a frequency of 440Hz.\n</code></pre> <p>Slice syntax can be used to query multiple subchannels:</p> <pre><code>square = SquareOscillator([440, 441, 442, 880])\noutput = square[0:2]\n# now contains a two-channel square wave\n</code></pre> <p>\u2192 Next: Status and properties</p>"},{"location":"node/operators/","title":"Nodes","text":""},{"location":"node/operators/#node-operators","title":"Node operators","text":""},{"location":"node/operators/#arithmetic","title":"Arithmetic","text":"<p>The output of multiple nodes can be combined using Python's mathematical operators. For example, to sum two sine waves together to create harmonics, use the <code>+</code> operator:</p> <pre><code>output = SineOscillator(440) + SineOscillator(880)\noutput.play()\n</code></pre> <p>To modulate the amplitude of one node with another, use the <code>*</code> operator:</p> <pre><code>sine = SineOscillator(440)\nenvelope = ASREnvelope(0.1, 1, 0.1)\noutput = sine * envelope\n</code></pre> <p>You can use constant values in place of <code>Node</code> objects:</p> <pre><code>sine = SineOscillator(440)\nattenuated = sine * 0.5\n</code></pre> <p>Operators can be chained together in the normal way:</p> <pre><code># Create an envelope that rises from 0.5 to 1.0 and back to 0.5\nenv = (ASREnvelope(0.1, 1, 0.1) * 0.5) + 0.5\n</code></pre> <p>Behind the scenes, these operators are actually creating composites of <code>Node</code> subclasses. The last example could alternatively be written as:</p> <pre><code>Add(Multiply(ASREnvelope(0.1, 1, 0.1), 0.5), 0.5)\n</code></pre>"},{"location":"node/operators/#comparison","title":"Comparison","text":"<p>Comparison operators can also be used to compare two Node output values, generating a binary (1/0) output. For example:</p> <pre><code># Generates an output of 1 when the sinusoid is above 0, and 0 otherwise \nSineOscillator(440) &gt; 0\n</code></pre> <p>This can then be used as an input to other nodes. The below will generate a half-wave-rectified sine signal (that is, a sine wave with all negative values set to zero). </p> <pre><code>sine = SineOscillator(440)\nrectified = sine * (sine &gt; 0)\n</code></pre>"},{"location":"node/operators/#index-of-operators","title":"Index of operators","text":"<p>Below is a full list of operators supported by SignalFlow.</p>"},{"location":"node/operators/#arithmetic-operators","title":"Arithmetic operators","text":"Operator Node class <code>+</code> Add <code>-</code> Subtract <code>*</code> Multiply <code>/</code> Divide <code>**</code> Power <code>%</code> Modulo"},{"location":"node/operators/#comparison-operators","title":"Comparison operators","text":"Operator Node class <code>==</code> Equal <code>!=</code> NotEqual <code>&lt;</code> LessThan <code>&lt;=</code> LessThanOrEqual <code>&gt;</code> GreaterThan <code>&gt;=</code> GreaterThanOrEqual <p>\u2192 Next: Multichannel</p>"},{"location":"node/playback/","title":"Nodes","text":""},{"location":"node/playback/#playing-and-stopping-a-node","title":"Playing and stopping a node","text":""},{"location":"node/playback/#starting-playback","title":"Starting playback","text":"<p>To start a node playing, simply call the <code>play()</code> method:</p> <pre><code>graph = AudioGraph()\nnode = SineOscillator(440)\nnode.play()\n</code></pre> <p>This connects the node to the <code>output</code> endpoint of the current global <code>AudioGraph</code>. The next time the graph processes a block of samples, the graph's <code>output</code> node then calls upon the sine oscillator to generate a block.</p> <p>It is important to remember that playing a node means \"connecting it to the graph\". For this reason, it is not possible to play the same node more than once, as it is already connected to the graph. To play multiples of a particular Node type, simply create and play multiple instances.</p>"},{"location":"node/playback/#connecting-a-node-to-another-nodes-input","title":"Connecting a Node to another Node's input","text":"<p>It is often the case that you want to connect a Node to the input of another Node for playback, rather than simply wiring it to the output of a graph -- for example, to pass an oscillator through a processor. In this case, you do not need to call <code>play()</code> (which means \"connect this node to the graph\"). Instead, it is sufficient to simply connect the Node to the input of another Node that is already playing.</p> <p>For example:</p> <pre><code># create and begin playback of a variable input summer, passed through a filter\nsum = Sum()\nflt = SVFilter(sum, \"low_pass\", 200)\nflt.play()\n</code></pre> <p>Now, let's create an oscillator. Observe that connecting the oscillator to the filter's input begins playback immediately.</p> <pre><code>square = SquareOscillator(100)\nsum.add_input(square)\n</code></pre>"},{"location":"node/playback/#stopping-playback","title":"Stopping playback","text":"<p>To stop a node playing:</p> <pre><code>node.stop()\n</code></pre> <p>This disconnects the node from the output device that it is connected to. </p> <p>\u2192 Next: Inputs</p>"},{"location":"node/properties/","title":"Nodes","text":""},{"location":"node/properties/#node-properties","title":"Node properties","text":"<p>A <code>Node</code> has a number of read-only properties which can be used to query its status at a given moment in time.</p> Property Type Description name str Short alphanumeric string that identifies the type of node (for example, <code>asr-envelope</code>) num_output_channels int The number of output channels that the node generates. num_input_channels int The number of input channels that the node takes. Note that most nodes have <code>matches_input_channels</code> set, meaning that their <code>num_input_channels</code> will be automatically increased according to their inputs. To learn more, see Nodes: Multichannel. matches_input_channels bool Whether the node automatically increases its <code>num_input_channels</code> based on its inputs. To learn more, see Nodes: Multichannel. has_variable_inputs bool Whether the node supports an arbitrary number of audio-rate inputs output_buffer numpy.ndarray Contains the Node's most recent audio output, in <code>float32</code> samples. The buffer is indexed by <code>channel</code> x <code>frame</code>, so to obtain the 32nd sample in the first channel, query: <code>node.output_buffer[0][31]</code>. inputs dict A dict containing all of the <code>Node</code>'s audio-rate inputs. Note that buffer inputs are not currently included within this dict. state int The Node's current playback state, which can be one of <code>SIGNALFLOW_NODE_STATE_ACTIVE</code> and <code>SIGNALFLOW_NODE_STATE_STOPPED</code>. The <code>STOPPED</code> state only applies to those nodes which have a finite duration (e.g. <code>ASREnvelope</code>, or <code>BufferPlayer</code> with looping disabled) and have reached the end of playback. Nodes continue to have a state of <code>ACTIVE</code> whether or not they are connected to the graph. patch Patch Indicates the Patch that the node is part of, or None if the Node does not belong to a Patch."},{"location":"node/properties/#monitoring-a-nodes-output","title":"Monitoring a node's output","text":"<p>To monitor the output of a node, call <code>node.poll(num_seconds)</code>, where <code>num_seconds</code> is the interval between messages. This will print the last sample generated by the node to <code>stdout</code>. In the case of multichannel nodes, only the first channel's value is printed.</p> <pre><code>&gt;&gt;&gt; a = Counter(Impulse(1))\n&gt;&gt;&gt; a.poll(1)\n&gt;&gt;&gt; a.play()\ncounter: 0.00000\ncounter: 1.00000\ncounter: 2.00000\n</code></pre> <p>To stop polling a node, call <code>node.poll(0)</code>.</p>"},{"location":"node/properties/#node-specific-properties","title":"Node-specific properties","text":"<p>Some <code>Node</code> classes have additional properties, containing information on implementation-specific states. These can be accessed via the <code>get_property</code> method.</p> <p>For example, the <code>BufferPlayer</code> node exposes a <code>position</code> property, which returns the playhead's current position, in seconds.</p> <pre><code>&gt;&gt;&gt; buffer = Buffer(\"audio.wav\")\n&gt;&gt;&gt; player = BufferPlayer(buffer)\n&gt;&gt;&gt; player.play()\n...\n&gt;&gt;&gt; player.get_property(\"position\")\n5.984000205993652\n</code></pre> <p>\u2192 Next: Stochastic nodes</p>"},{"location":"node/stochastic/","title":"Nodes","text":""},{"location":"node/stochastic/#chance-and-stochastic-nodes","title":"Chance and stochastic nodes","text":"<p>SignalFlow has a number of stochastic nodes, which make use of a pseudo-random number generator (RNG) to produce unpredictable output values.</p> <p>Each object of these <code>StochasticNode</code> subclasses stores its own RNG. By default, the RNG is seeded with a random value, so that each run will generate a different set of outputs. However, to create a repeatable pseudo-random output, the <code>seed</code> of the node's RNG can be set to a known value:</p> <pre><code>&gt;&gt;&gt; r = RandomUniform(0, 1)\n&gt;&gt;&gt; r.process(1024)\n&gt;&gt;&gt; r.output_buffer[0][:4]\narray([0.48836085, 0.64326525, 0.79819506, 0.8489549 ], dtype=float32)\n&gt;&gt;&gt; r.set_seed(123)\n&gt;&gt;&gt; r.process(1024)\n&gt;&gt;&gt; r.output_buffer[0][:4]\narray([0.7129553 , 0.42847094, 0.6908848 , 0.7191503 ], dtype=float32)\n&gt;&gt;&gt; r.set_seed(123)\n&gt;&gt;&gt; r.process(1024)\n&gt;&gt;&gt; r.output_buffer[0][:4]\narray([0.7129553 , 0.42847094, 0.6908848 , 0.7191503 ], dtype=float32)\n</code></pre> <p>Note the identical sequences generated after repeatedly setting the seed to a known value. </p> <p>Warning</p> <p>Calling <code>node.process()</code> is generally not good practice, as it does not recursively process all of the node's inputs (unlike when a node is embedded within an AudioGraph, which correctly handles recursion and cyclical loops). Please use at your peril!</p> <p>\u2192 Next: Node reference library</p>"},{"location":"patch/","title":"Patch","text":"<p>Warning</p> <p>This documentation is a work-in-progress and may have sections that are missing or incomplete.</p> <p>A <code>Patch</code> represents a connected group of <code>Nodes</code>, analogous to a synthesizer. Defining patches makes it easy to create higher-level structures, which can then be reused and instantiated with a single line of code, in much the same way as a Node.</p> <p>Behind the scenes, the structure of a <code>Patch</code> is encapsulated by a <code>PatchSpec</code>, a template which can be instantiated or serialised to a JSON file for later use.  </p> <ul> <li>A Patch structure is defined either by declaring a Patch subclass or with a JSON specification file</li> <li>Play and stop a Patch by connecting it to the AudioGraph or the input of another Patch or Node </li> <li>Similar to nodes, a Patch can be modulated by audio-rate inputs, triggered by trigger inputs, and access sample data via buffer inputs </li> <li>The outputs of Patches can be altered or combined by normal Python operators</li> <li>The status of a Patch can be queried via its properties </li> <li>Patches can be exported and imported to JSON</li> <li>The auto-free mechanism allows Patches to automatically stop and free their memory after playback is complete </li> </ul> <p>\u2192 Next: Defining a Patch</p>"},{"location":"patch/auto-free/","title":"Patch","text":""},{"location":"patch/auto-free/#auto-free-and-memory-management","title":"Auto-free and memory management","text":"<p>Auto-free.</p>"},{"location":"patch/defining/","title":"Patch","text":""},{"location":"patch/defining/#defining-a-patch","title":"Defining a Patch","text":"<p>A Patch is made up of a connected network of Nodes, together with a set of properties that determine how the Patch can be controlled.</p> <p>There are two general ways to define the structure of a Patch:</p> <ul> <li>Create a new class that subclasses <code>Patch</code>. In general, this is the recommended approach for defining new Patches.</li> <li>Create a JSON file that can be loaded as a <code>PatchSpec</code>, which describes the structure of a patch </li> </ul>"},{"location":"patch/defining/#creating-a-patch-subclass","title":"Creating a Patch subclass","text":"<p>The quickest and most intuitive way to define a <code>Patch</code> is by subclassing the <code>Patch</code> class itself. Let's look at an example.</p> <pre><code>class Bleep (Patch):\n    def __init__(self, frequency=880, duration=0.1):\n        super().__init__()\n        frequency = self.add_input(\"frequency\", frequency)\n        duration = self.add_input(\"duration\", duration)\n        sine = SineOscillator(frequency)\n        env = ASREnvelope(0.001, duration, 0.001)\n        output = sine * env\n        self.set_output(output)\n        self.set_auto_free(True)\n</code></pre> <p>In the above example:</p> <ul> <li>At the very start of the <code>__init__</code> function, <code>super().__init__()</code> must be called to initialise the Patch and its storage. This is vital! Without it, your program will crash. </li> <li>Two audio-rate input parameters are defined. The <code>add_input()</code> method is used to define them as inputs of the <code>Patch</code>, which can then be subsequently modulated. Note that the <code>add_input()</code> method returns a reference to the frequency node, which then acts as a pointer to the input node.</li> <li><code>self.set_output()</code> is used to define the Patch's output. A Patch can only have one single output.</li> <li>Finally, <code>self.set_auto_free()</code> is used to automatically stop and free the Patch after playback of the envelope is completed. More about auto-free... </li> </ul> <p>You can now instantiate a <code>Bleep</code> object in just the same way as you would instantiate and play a Node:</p> <pre><code>b = Bleep(frequency=440, duration=0.2)\nb.play()\n</code></pre> <p>If you query <code>graph.status</code> after playback has finished, you should see that the <code>Patch</code> is automatically freed and the number of nodes returns to 0. </p>"},{"location":"patch/defining/#creating-a-patchspec-from-json","title":"Creating a PatchSpec from JSON","text":"<p>The structure of a <code>Patch</code> is described by a <code>PatchSpec</code>, which can in turn be imported/exported in the JSON text-based data interchange format. </p> <p>For information on loading or saving PatchSpecs as JSON, see Exporting and importing patches.</p> <p>\u2192 Next: Playing and stopping a Patch</p>"},{"location":"patch/exporting/","title":"Patch","text":""},{"location":"patch/exporting/#exporting-and-importing-patches","title":"Exporting and importing patches","text":"<p>A Patch can be exported or imported.</p> <p>\u2192 Next: Auto-free and memory management</p>"},{"location":"patch/inputs/","title":"Patch","text":""},{"location":"patch/inputs/#patch-inputs","title":"Patch inputs","text":"<p>Just like a Node, a Patch supports three different classes of input:</p> <ul> <li>Audio-rate inputs: Takes the output of another Node or Patch as an input, for continuous modulation of synthesis parameters</li> <li>Trigger inputs: Used to trigger discrete control events \u2014 for example, restarting buffer playback</li> <li>Buffer inputs: Used to pass the contents of an audio buffer to a patch \u2014 for example, as a source of audio samples, or an envelope shape</li> </ul>"},{"location":"patch/inputs/#audio-rate-inputs","title":"Audio-rate inputs","text":"<p>A Patch supports any number of user-defined named inputs, which can be used to modulate the nodes within the patch.</p> <p>Each input must be defined by calling <code>add_input()</code> when the Patch is first defined, with an optional default value.</p> <p>Info</p> <p>Note that Patches do not yet support variable inputs.</p> <p>When a Patch is playing, the value of its inputs can be set using <code>patch.set_input()</code>:</p> <pre><code>class Bloop (Patch):\n    def __init__(self, frequency=880, duration=0.1):\n        super().__init__()\n        frequency = self.add_input(\"frequency\", frequency)\n        sine = SineOscillator(frequency)\n        self.set_output(sine)\n        self.set_auto_free(True)\n\nbloop = Bloop()\nbloop.play()\n...\nbloop.set_input(\"frequency\", 100)\n</code></pre> <p>Info</p> <p>Note that Patches do not yet support setting inputs with Python properties (e.g. <code>patch.prop_name = 123</code>), as is possible with node inputs.</p>"},{"location":"patch/inputs/#triggers","title":"Triggers","text":"<p>When defining a <code>Patch</code>, it is possible to define which Node should receive <code>trigger()</code> events sent to the Patch. This is done with <code>patch.set_trigger_node()</code>:</p> <pre><code>class Hat (Patch):\n    def __init__(self, duration=0.1):\n        super().__init__()\n        duration = self.add_input(\"duration\", duration)\n        noise = WhiteNoise()\n        env = ASREnvelope(0.0001, 0.0, duration, curve=2)\n        output = noise * env\n        self.set_trigger_node(env)\n        self.set_output(output)\n\nh = Hat()\nh.play()\n...\nh.trigger() # triggers a hit, resetting the ASREnvelope to its start point\n</code></pre> <p>This can be used to create a <code>Patch</code> that stays connected to the AudioGraph and can be retriggered to play a hit.</p> <p>Info</p> <p>Note that Patches only presently support trigger events directed to a single node within the patch, and cannot route triggers to multiple different nodes.</p>"},{"location":"patch/inputs/#buffer-inputs","title":"Buffer inputs","text":"<p>Buffer inputs can be declared at define time by calling <code>self.add_buffer_input()</code>. Similar to <code>add_input</code>, the return value is a placeholder <code>Buffer</code> that can be used wherever you would normally pass a <code>Buffer</code>:</p> <pre><code>class WobblyPlayer (Patch):\n    def __init__(self, buffer):\n        super().__init__()\n        buffer = self.add_buffer_input(\"buffer\", buffer)\n        rate = SineLFO(0.2, 0.5, 1.5)\n        player = BufferPlayer(buffer, rate=rate, loop=True)\n        self.set_output(player)\n\nbuffer = Buffer(\"examples/audio/stereo-count.wav\")\nplayer = WobblyPlayer(buffer)\nplayer.play()\n</code></pre> <p>The buffer can then be replaced at runtime by calling <code>set_input()</code>:</p> <pre><code>player.set_input(\"buffer\", another_buffer)\n</code></pre> <p>\u2192 Next: Operators</p>"},{"location":"patch/operators/","title":"Patch","text":""},{"location":"patch/operators/#operators","title":"Operators","text":"<p>The output of a Patch can be amplified, attenuated, combined, modulated and compared using Python operators, in much the same way as Node:</p> <pre><code>patch = Patch(patch_spec)\noutput = patch * 0.5\n</code></pre> <p>For a full list of the operators that can be applied to a <code>Patch</code>, see Node operators.</p> <p>\u2192 Next: Patch properties</p>"},{"location":"patch/playback/","title":"Patch","text":""},{"location":"patch/playback/#playing-a-patch","title":"Playing a Patch","text":"<p>Once a <code>Patch</code> has been defined or imported, it can be instantiated in two different ways depending on how it was defined:</p> <ul> <li>From a Patch subclass</li> <li>From a PatchSpec</li> </ul>"},{"location":"patch/playback/#from-a-patch-subclass","title":"From a Patch subclass","text":"<p>The simplest way to instantiate a Patch is by defining it as a Patch subclass, and then instantiating it in the same way as a Node. </p> <pre><code>class Hat (Patch):\n    def __init__(self, duration=0.1):\n        super().__init__()\n        duration = self.add_input(\"duration\", duration)\n        noise = WhiteNoise()\n        env = ASREnvelope(0.0001, 0.0, duration, curve=2)\n        output = noise * env\n        self.set_output(output)\n        self.set_auto_free(True)\n\nhat = Hat()\nhat.play()\n</code></pre> <p>Once a Patch has finished, its state changes to <code>SIGNALFLOW_PATCH_STATE_STOPPED</code>.</p> <p>Just as with nodes, it is important to remember that playing a patch means \"connecting it to the graph\". For this reason, it is not possible to play the same patch more than once, as it is already connected to the graph.</p> <p>To play multiples of a particular <code>Patch</code> type, simply create and play multiple instances.</p>"},{"location":"patch/playback/#from-a-patchspec","title":"From a PatchSpec","text":"<p>Once a <code>PatchSpec</code> has been created or imported, it can be played by instantiating a <code>Patch</code> with the <code>PatchSpec</code> as an argument:</p> <pre><code>patch = Patch(patch_spec)\npatch.play()\n</code></pre>"},{"location":"patch/playback/#connecting-a-patch-to-another-patchs-input","title":"Connecting a Patch to another Patch's input","text":"<p>A <code>Patch</code> can be connected to the input of another <code>Patch</code> (or Node), in exactly the same way described in Connecting a Node to another Node's input.</p> <p>Once you have got to grips with this paradigm, it becomes simple to build up sophisticated processing graphs by abstracting complex functionality within individual <code>Patch</code> objects, and connecting them to one another. </p>"},{"location":"patch/playback/#stopping-a-patch","title":"Stopping a Patch","text":"<p>As in Node playback, stopping a Patch disconnects it from the AudioGraph. Patches with auto-free are automatically stopped when their lifetimes ends. Patches with an unlimited lifespan must be stopped manually, with:</p> <pre><code>patch.stop()\n</code></pre> <p>This disconnects the Patch from its output.</p> <p>\u2192 Next: Patch inputs</p>"},{"location":"patch/properties/","title":"Patch","text":""},{"location":"patch/properties/#patch-properties","title":"Patch properties","text":"Property Type Description nodes list A list of all of the Node objects that make up this Patch inputs dict A dict of key-value pairs corresponding to all of the (audio rate) inputs within the Patch state int The Patch's current playback state, which can be <code>SIGNALFLOW_PATCH_STATE_ACTIVE</code> or <code>SIGNALFLOW_PATCH_STATE_STOPPED</code>. graph AudioGraph A reference to the AudioGraph that the Patch is part of <p>\u2192 Next: Exporting and importing patches</p>"},{"location":"planning/NAMING/","title":"NAMING","text":""},{"location":"planning/NAMING/#nodes","title":"NODES","text":"<p>Generators  - Oscillators    - Wavetable    - Waveforms (all wrappers around Wavetable with band-limiting)      - SineOscillator      - SquareOscillator      - TriangleOscillator      - SawOscillator  - LFO (all wrappers around Wavetable)    - SineLFO    - SquareLFO    - TriangleLFO    - SawLFO  - Buffer    - BufferPlayer    - BufferRecorder  - Stochastic    -  Processors  - Panners    - ChannelMixer    - LinearPanner    - AzimuthPanner    - ObjectPanner  - Delay    - AllpassDelay    - Delay  - Effects    - EQ    - Gate    - Resampler    - Waveshaper</p> <p>Stochastic  - Random signal generators    - WhiteNoise    - PinkNoise     - BrownNoise    - PerlinNoise  - Random number generators (with clocked inputs)    - RandomUniform    - RandomLinear    - RandomBrownian    - RandomExponentialDist    - RandomGaussian    - RandomBeta  - Random event generators    - RandomImpulse</p> <p>-- PATCHES  - Patch  - PatchDef</p>"}]}