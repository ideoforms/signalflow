{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SignalFlow Warning This documentation is a work-in-progress and may have sections that are missing or incomplete. SignalFlow is an audio DSP framework whose goal is to make it quick and intuitive to explore complex sonic ideas. It has a simple and consistent Python API, allowing for rapid prototyping in Jupyter, PyCharm, or on the command-line. It comes with over 100 of built-in node classes for creative exploration. Its core is implemented in C++11, with cross-platform hardware acceleration. SignalFlow has robust support for macOS and Linux (including Raspberry Pi), and has work-in-progress support for Windows. The overall project is currently in alpha status, and interfaces may change without warning. This documentation currently focuses specifically on Python interfaces and examples. Overview At its core, SignalFlow has a handful of key concepts. At the top level is the AudioGraph , which connects to the system's audio input/output hardware. The graph comprises of a network of Nodes , each of which performs a single function (for example, generating a cyclical waveform, or filtering an input node). Nodes are connected by input and output relationships: the output of one node may be used to control the frequency of another. As the output of the first node increases, the frequency of the second node increases correspondingly. This modulation is applied on a sample-by-sample basis: all modulation in SignalFlow happens at audio rate. Nodes may have multiple inputs , which determine which synthesis properties can be modulated at runtime. A node can also have Buffer properties, which contain audio waveform data that can be read and written to, for playback or recording of samples. Nodes can be grouped in a Patch , which is a user-defined configuration of nodes. A patch may have one or more named inputs that are defined by the user when creating the patch. Patches can be thought of like voices of a synthesizer. A patch can also be set to automatically remove itself from the graph when a specified node's playback is complete, which is important for automatic memory management. Example Let's take a look at a minimal SignalFlow example. Here, we create and immediately start the AudioGraph , construct a stereo sine oscillator, connect the oscillator to the graph's output, and run the graph indefinitely. from signalflow import * graph = AudioGraph () sine = SineOscillator ([ 440 , 880 ]) envelope = ASREnvelope ( 0.1 , 0.1 , 0.5 ) output = sine * envelope output . play () graph . wait () This demo shows a few syntactical benefits that SignalFlow provides to make it easy to work with audio: The 2-item array of frequency values passed to SineOscillator is expanded to create a stereo, 2-channel output. If you passed a 10-item array, the output would have 10 channels. ( Read more: Multichannel nodes ) Mathematical operators like * can be used to multiply, add, subtract or divide the output of nodes, and creates a new output Node that corresponds to the output of the operation. This example uses an envelope to modulate the amplitude of an oscillator. ( Read more: Node operators ) Even through the envelope is mono and the oscillator is stereo, SignalFlow does the right thing and upmixes the envelope's values to create a stereo output, so that the same envelope shape is applied to the L and R channels of the oscillator, before creating a stereo output. This is called \"automatic upmixing\", and is handy when working with multichannel graphs. ( Read more: Automatic upmixing ) In subsequent examples, we will skip the import line and assume you have already imported everything from the signalflow namespace. Info If you want to keep your namespaces better separated, you might want to do something like the below. import signalflow as sf graph = sf . AudioGraph () sine = sf . SineOscillator ( 440 ) ... Documentation Getting started Example code","title":"Home"},{"location":"#signalflow","text":"Warning This documentation is a work-in-progress and may have sections that are missing or incomplete. SignalFlow is an audio DSP framework whose goal is to make it quick and intuitive to explore complex sonic ideas. It has a simple and consistent Python API, allowing for rapid prototyping in Jupyter, PyCharm, or on the command-line. It comes with over 100 of built-in node classes for creative exploration. Its core is implemented in C++11, with cross-platform hardware acceleration. SignalFlow has robust support for macOS and Linux (including Raspberry Pi), and has work-in-progress support for Windows. The overall project is currently in alpha status, and interfaces may change without warning. This documentation currently focuses specifically on Python interfaces and examples.","title":"SignalFlow"},{"location":"#overview","text":"At its core, SignalFlow has a handful of key concepts. At the top level is the AudioGraph , which connects to the system's audio input/output hardware. The graph comprises of a network of Nodes , each of which performs a single function (for example, generating a cyclical waveform, or filtering an input node). Nodes are connected by input and output relationships: the output of one node may be used to control the frequency of another. As the output of the first node increases, the frequency of the second node increases correspondingly. This modulation is applied on a sample-by-sample basis: all modulation in SignalFlow happens at audio rate. Nodes may have multiple inputs , which determine which synthesis properties can be modulated at runtime. A node can also have Buffer properties, which contain audio waveform data that can be read and written to, for playback or recording of samples. Nodes can be grouped in a Patch , which is a user-defined configuration of nodes. A patch may have one or more named inputs that are defined by the user when creating the patch. Patches can be thought of like voices of a synthesizer. A patch can also be set to automatically remove itself from the graph when a specified node's playback is complete, which is important for automatic memory management.","title":"Overview"},{"location":"#example","text":"Let's take a look at a minimal SignalFlow example. Here, we create and immediately start the AudioGraph , construct a stereo sine oscillator, connect the oscillator to the graph's output, and run the graph indefinitely. from signalflow import * graph = AudioGraph () sine = SineOscillator ([ 440 , 880 ]) envelope = ASREnvelope ( 0.1 , 0.1 , 0.5 ) output = sine * envelope output . play () graph . wait () This demo shows a few syntactical benefits that SignalFlow provides to make it easy to work with audio: The 2-item array of frequency values passed to SineOscillator is expanded to create a stereo, 2-channel output. If you passed a 10-item array, the output would have 10 channels. ( Read more: Multichannel nodes ) Mathematical operators like * can be used to multiply, add, subtract or divide the output of nodes, and creates a new output Node that corresponds to the output of the operation. This example uses an envelope to modulate the amplitude of an oscillator. ( Read more: Node operators ) Even through the envelope is mono and the oscillator is stereo, SignalFlow does the right thing and upmixes the envelope's values to create a stereo output, so that the same envelope shape is applied to the L and R channels of the oscillator, before creating a stereo output. This is called \"automatic upmixing\", and is handy when working with multichannel graphs. ( Read more: Automatic upmixing ) In subsequent examples, we will skip the import line and assume you have already imported everything from the signalflow namespace. Info If you want to keep your namespaces better separated, you might want to do something like the below. import signalflow as sf graph = sf . AudioGraph () sine = sf . SineOscillator ( 440 ) ...","title":"Example"},{"location":"#documentation","text":"Getting started Example code","title":"Documentation"},{"location":"examples/","text":"Examples For various code examples using SignalFlow, see examples/python in GitHub: https://github.com/ideoforms/signalflow/tree/master/examples/python","title":"Examples"},{"location":"examples/#examples","text":"For various code examples using SignalFlow, see examples/python in GitHub: https://github.com/ideoforms/signalflow/tree/master/examples/python","title":"Examples"},{"location":"getting-started/","text":"Getting started Installation SignalFlow is not yet available for installation via pip. You will currently need to clone the GitHub repo and build and run from the command line, which is hopefully a quick and easy process. For details, see the README in the repo: https://github.com/ideoforms/signalflow Examples Several example scripts are included within the repo, covering simple control and modulation, FM synthesis, sample granulation, MIDI control, chaotic functions, etc.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"","title":"Getting started"},{"location":"getting-started/#installation","text":"SignalFlow is not yet available for installation via pip. You will currently need to clone the GitHub repo and build and run from the command line, which is hopefully a quick and easy process. For details, see the README in the repo: https://github.com/ideoforms/signalflow","title":"Installation"},{"location":"getting-started/#examples","text":"Several example scripts are included within the repo, covering simple control and modulation, FM synthesis, sample granulation, MIDI control, chaotic functions, etc.","title":"Examples"},{"location":"license/","text":"License SignalFlow is under the MIT license . This means that you are welcome to use it for any purpose, including commercial usage, but must include the copyright notice above in any copies or derivative works. Please do let me know what you use it for!","title":"License"},{"location":"license/#license","text":"SignalFlow is under the MIT license . This means that you are welcome to use it for any purpose, including commercial usage, but must include the copyright notice above in any copies or derivative works. Please do let me know what you use it for!","title":"License"},{"location":"buffer/","text":"Buffer Warning This documentation is a work-in-progress and may have sections that are missing or incomplete. A Buffer is an allocated area of memory that can be used to store single-channel or multi-channel data, which may represent an audio waveform or any other type of signal. A Buffer can be created from a sound file, an array of samples, or with an empty contents A Buffer can be passed to a Node or Patch as an input A Buffer can be exported to a sound file A Buffer's data can be directly accessed in memory as a numpy array, or by get/set methods The contents of a buffer can be combined with arithmetic operators Properties Buffer interpolation modes 2D buffers Buffer applications: Sample recording and playback, control recording and playback, envelopes, waveshapers The total Buffer memory usage can be queried","title":"Buffers"},{"location":"buffer/#buffer","text":"Warning This documentation is a work-in-progress and may have sections that are missing or incomplete. A Buffer is an allocated area of memory that can be used to store single-channel or multi-channel data, which may represent an audio waveform or any other type of signal. A Buffer can be created from a sound file, an array of samples, or with an empty contents A Buffer can be passed to a Node or Patch as an input A Buffer can be exported to a sound file A Buffer's data can be directly accessed in memory as a numpy array, or by get/set methods The contents of a buffer can be combined with arithmetic operators Properties Buffer interpolation modes 2D buffers Buffer applications: Sample recording and playback, control recording and playback, envelopes, waveshapers The total Buffer memory usage can be queried","title":"Buffer"},{"location":"graph/","text":"The AudioGraph AudioGraph is the global audio processing graph that schedules and performs the audio processing. It contains references to all of the Node and Patch objects that are currently active, and, when a new block of audio is requested by the system audio I/O layer, traverses the tree of nodes and generates new samples. \u2192 Next: Creating the graph","title":"The AudioGraph"},{"location":"graph/#the-audiograph","text":"AudioGraph is the global audio processing graph that schedules and performs the audio processing. It contains references to all of the Node and Patch objects that are currently active, and, when a new block of audio is requested by the system audio I/O layer, traverses the tree of nodes and generates new samples. \u2192 Next: Creating the graph","title":"The AudioGraph"},{"location":"graph/config/","text":"The AudioGraph Graph configuration There are a number of graph configuration parameters that can be used to change the global behaviour of the audio system. Parameter Description output_device_name The name of the audio output device to use. This must precisely match the device's name in your system. If not found, DeviceNotFoundException is thrown when instantiating the graph. input_device_name The name of the input device to use. sample_rate The audio sample rate to use. output_buffer_size The size of the hardware output audio buffer, in samples. A larger buffer reduces the chance of buffer overflows and glitches, but at the cost of higher latency. Note that this config option merely specifies the preferred output buffer size, which may not be available in the system hardware. To check the actual buffer size used by the AudioGraph, query graph.output_buffer_size after instantiation. input_buffer_size The size of the hardware input audio buffer. cpu_usage_limit Imposes a hard limit on the CPU usage permitted by SignalFlow. If the estimated (single-core) CPU usage exceeds this value, no more nodes or patches can be created until it returns to below the limit. Floating-point value between 0..1, where 0.5 means 50% CPU. Configuring the graph programmatically To specify an alternative config, create and populate an AudioGraphConfig object before the graph is started: config = AudioGraphConfig() config.output_device_name = \"MacBook Pro Speakers\" config.sample_rate = 44100 config.output_buffer_size = 2048 graph = AudioGraph(config) Configuring the graph via ~/.signalflow/config To specify a configuration that is used by all future SignalFlow sessions, create a file ~/.signalflow/config with the following format: [audio] sample_rate = 48000 output_buffer_size = 256 input_buffer_size = 256 output_device_name = \"MacBook Pro Speakers\" input_device_name = \"MacBook Pro Microphone\" All fields are optional. \u2192 Next: Graph status and properties","title":"Configuration"},{"location":"graph/config/#the-audiograph","text":"","title":"The AudioGraph"},{"location":"graph/config/#graph-configuration","text":"There are a number of graph configuration parameters that can be used to change the global behaviour of the audio system. Parameter Description output_device_name The name of the audio output device to use. This must precisely match the device's name in your system. If not found, DeviceNotFoundException is thrown when instantiating the graph. input_device_name The name of the input device to use. sample_rate The audio sample rate to use. output_buffer_size The size of the hardware output audio buffer, in samples. A larger buffer reduces the chance of buffer overflows and glitches, but at the cost of higher latency. Note that this config option merely specifies the preferred output buffer size, which may not be available in the system hardware. To check the actual buffer size used by the AudioGraph, query graph.output_buffer_size after instantiation. input_buffer_size The size of the hardware input audio buffer. cpu_usage_limit Imposes a hard limit on the CPU usage permitted by SignalFlow. If the estimated (single-core) CPU usage exceeds this value, no more nodes or patches can be created until it returns to below the limit. Floating-point value between 0..1, where 0.5 means 50% CPU.","title":"Graph configuration"},{"location":"graph/config/#configuring-the-graph-programmatically","text":"To specify an alternative config, create and populate an AudioGraphConfig object before the graph is started: config = AudioGraphConfig() config.output_device_name = \"MacBook Pro Speakers\" config.sample_rate = 44100 config.output_buffer_size = 2048 graph = AudioGraph(config)","title":"Configuring the graph programmatically"},{"location":"graph/config/#configuring-the-graph-via-signalflowconfig","text":"To specify a configuration that is used by all future SignalFlow sessions, create a file ~/.signalflow/config with the following format: [audio] sample_rate = 48000 output_buffer_size = 256 input_buffer_size = 256 output_device_name = \"MacBook Pro Speakers\" input_device_name = \"MacBook Pro Microphone\" All fields are optional. \u2192 Next: Graph status and properties","title":"Configuring the graph via ~/.signalflow/config"},{"location":"graph/creating/","text":"The AudioGraph Creating the graph Creating the graph is simple: graph = AudioGraph() By default, a new AudioGraph immediately connects to the system's default audio hardware device (via the integrated libsoundio library), using the system's default sample rate and buffer size. Info Note that the AudioGraph is a singleton object: only one AudioGraph can be created, which is shared globally. To prevent the graph from starting instantly (for example, if you want to use the graph in offline mode), pass start=False to the constructor. To configure graph playback or recording parameters, see AudioGraph: Configuration . \u2192 Next: Graph configuration","title":"Creating the graph"},{"location":"graph/creating/#the-audiograph","text":"","title":"The AudioGraph"},{"location":"graph/creating/#creating-the-graph","text":"Creating the graph is simple: graph = AudioGraph() By default, a new AudioGraph immediately connects to the system's default audio hardware device (via the integrated libsoundio library), using the system's default sample rate and buffer size. Info Note that the AudioGraph is a singleton object: only one AudioGraph can be created, which is shared globally. To prevent the graph from starting instantly (for example, if you want to use the graph in offline mode), pass start=False to the constructor. To configure graph playback or recording parameters, see AudioGraph: Configuration . \u2192 Next: Graph configuration","title":"Creating the graph"},{"location":"graph/properties/","text":"The AudioGraph Status and properties A number of methods are provided to query the graph's current status and properties. Status Querying graph.status returns a one-line description of the number of nodes and patches in the graph, and the estimated CPU and RAM usage: >>> graph.status AudioGraph: 235 active nodes, 6 patches, 13.95% CPU usage, 34.91MB memory usage To automatically poll and print the graph's status periodically, call graph.poll(interval) , where interval is in seconds: >>> graph.poll(1) AudioGraph: 118 active nodes, 3 patches, 7.09% CPU usage, 34.91MB memory usage AudioGraph: 118 active nodes, 3 patches, 7.16% CPU usage, 34.91MB memory usage AudioGraph: 40 active nodes, 1 patch, 2.60% CPU usage, 34.91MB memory usage To stop polling, call graph.poll(0) . Structure Querying graph.structure returns a multi-line string describing every Node in the graph, their parameter values, and their connectivity structure. >>> graph.structure * audioout-soundio input0: * linear-panner pan: 0.000000 input: * multiply input1: 0.251189 input0: * sine frequency: 440.000000 Other graph properties graph.node_count (int): Returns the current number of Nodes in the graph (including within patches) graph.patch_count (int): Returns the current number of Patches in the graph cpu_usage (float): Returns the current CPU usage, between 0.0 (0%) and 1.0 (100%). CPU usage can be lowered by increasing the output buffer size. memory_usage (int): Returns the current RAM usage, in bytes. This is typically mostly used by waveform data in Buffers . num_output_channels (int): Returns the graph's current output channel count, which is typically identical to the number of channels supported by the audio output device. output_buffer_size (int): Returns the current hardware output buffer size, in bytes. \u2192 Next: Recording graph output","title":"Status and properties"},{"location":"graph/properties/#the-audiograph","text":"","title":"The AudioGraph"},{"location":"graph/properties/#status-and-properties","text":"A number of methods are provided to query the graph's current status and properties.","title":"Status and properties"},{"location":"graph/properties/#status","text":"Querying graph.status returns a one-line description of the number of nodes and patches in the graph, and the estimated CPU and RAM usage: >>> graph.status AudioGraph: 235 active nodes, 6 patches, 13.95% CPU usage, 34.91MB memory usage To automatically poll and print the graph's status periodically, call graph.poll(interval) , where interval is in seconds: >>> graph.poll(1) AudioGraph: 118 active nodes, 3 patches, 7.09% CPU usage, 34.91MB memory usage AudioGraph: 118 active nodes, 3 patches, 7.16% CPU usage, 34.91MB memory usage AudioGraph: 40 active nodes, 1 patch, 2.60% CPU usage, 34.91MB memory usage To stop polling, call graph.poll(0) .","title":"Status"},{"location":"graph/properties/#structure","text":"Querying graph.structure returns a multi-line string describing every Node in the graph, their parameter values, and their connectivity structure. >>> graph.structure * audioout-soundio input0: * linear-panner pan: 0.000000 input: * multiply input1: 0.251189 input0: * sine frequency: 440.000000","title":"Structure"},{"location":"graph/properties/#other-graph-properties","text":"graph.node_count (int): Returns the current number of Nodes in the graph (including within patches) graph.patch_count (int): Returns the current number of Patches in the graph cpu_usage (float): Returns the current CPU usage, between 0.0 (0%) and 1.0 (100%). CPU usage can be lowered by increasing the output buffer size. memory_usage (int): Returns the current RAM usage, in bytes. This is typically mostly used by waveform data in Buffers . num_output_channels (int): Returns the graph's current output channel count, which is typically identical to the number of channels supported by the audio output device. output_buffer_size (int): Returns the current hardware output buffer size, in bytes. \u2192 Next: Recording graph output","title":"Other graph properties"},{"location":"graph/recording/","text":"The AudioGraph Recording the audio output of the graph Convenience methods are provided to make it easy to record the global audio output when rendering audio in real-time: graph . start_recording ( \"filename.wav\" ) ... graph . stop_recording () To record output in formats other than the default stereo, start_recording takes a num_channels argument that can be used to specify an alternative channel count. Note At present, only .wav is supported as an output format for global audio recordings. Offline (non-real-time) rendering It is also possible to perform non-real-time rendering of a synthesis graph, by synthesizing audio output to a Buffer which can then be saved to disk: # Create an AudioGraph with a dummy output device graph = AudioGraph ( output_device = AudioOut_Dummy ( 2 )) # Create a buffer that will be used to store the audio output buffer = Buffer ( 2 , graph . sample_rate * 4 ) # Create a synthesis graph to render freq = SawLFO ( 1 , 200 , 400 ) sine = SineOscillator ([ freq , freq + 10 ]) graph . play ( sine ) # Render to the buffer. Non-real-time, so happens instantaneously. graph . render_to_buffer ( buffer ) # Write the buffer contents to a file buffer . save ( \"output.wav\" ) # Finally, tear down the buffer graph . destroy () \u2192 Next: Clearing and stopping the graph","title":"Recording the audio output"},{"location":"graph/recording/#the-audiograph","text":"","title":"The AudioGraph"},{"location":"graph/recording/#recording-the-audio-output-of-the-graph","text":"Convenience methods are provided to make it easy to record the global audio output when rendering audio in real-time: graph . start_recording ( \"filename.wav\" ) ... graph . stop_recording () To record output in formats other than the default stereo, start_recording takes a num_channels argument that can be used to specify an alternative channel count. Note At present, only .wav is supported as an output format for global audio recordings.","title":"Recording the audio output of the graph"},{"location":"graph/recording/#offline-non-real-time-rendering","text":"It is also possible to perform non-real-time rendering of a synthesis graph, by synthesizing audio output to a Buffer which can then be saved to disk: # Create an AudioGraph with a dummy output device graph = AudioGraph ( output_device = AudioOut_Dummy ( 2 )) # Create a buffer that will be used to store the audio output buffer = Buffer ( 2 , graph . sample_rate * 4 ) # Create a synthesis graph to render freq = SawLFO ( 1 , 200 , 400 ) sine = SineOscillator ([ freq , freq + 10 ]) graph . play ( sine ) # Render to the buffer. Non-real-time, so happens instantaneously. graph . render_to_buffer ( buffer ) # Write the buffer contents to a file buffer . save ( \"output.wav\" ) # Finally, tear down the buffer graph . destroy () \u2192 Next: Clearing and stopping the graph","title":"Offline (non-real-time) rendering"},{"location":"graph/stopping/","text":"The AudioGraph Clearing and stopping the graph To clear all nodes and patches from the graph but leave it running for further audio synthesis: >>> graph.clear() To stop the graph and pause audio I/O: >>> graph.stop() To permanently destroy the graph: >>> graph.destroy()","title":"Stopping the graph"},{"location":"graph/stopping/#the-audiograph","text":"","title":"The AudioGraph"},{"location":"graph/stopping/#clearing-and-stopping-the-graph","text":"To clear all nodes and patches from the graph but leave it running for further audio synthesis: >>> graph.clear() To stop the graph and pause audio I/O: >>> graph.stop() To permanently destroy the graph: >>> graph.destroy()","title":"Clearing and stopping the graph"},{"location":"howto/","text":"Howto Warning This documentation is a work-in-progress and may have sections that are missing or incomplete. Tutorials on common tasks with SignalFlow.","title":"Howto"},{"location":"howto/#howto","text":"Warning This documentation is a work-in-progress and may have sections that are missing or incomplete. Tutorials on common tasks with SignalFlow.","title":"Howto"},{"location":"howto/midi/","text":"Howto: MIDI control","title":"Howto: MIDI control"},{"location":"howto/midi/#howto-midi-control","text":"","title":"Howto: MIDI control"},{"location":"node/","text":"Nodes A Node object is an audio processing unit that performs one single function. For example, a Node's role may be to synthesize a waveform, read from a buffer, or take two input Nodes and sum their values. Nodes are played and stopped by connecting them to the AudioGraph A node has one or more audio-rate inputs , which can be modulated by other nodes Some nodes can be triggered with trigger inputs \u2014 for example, to restart playback, or set the position of an envelope Some nodes can be used to play back the contents of buffer inputs , or can use buffer data as a source of modulation The output of multiple nodes can be combined and modulated with use of the standard Python operators ( + , - , * , % , etc) The output of a node can be mono (single-channel) or multichannel A Node's status and output can be examined by querying its properties Some Nodes generate unpredictable stochastic output , which can be controlled via its internal random number generator Details of how to create a new Node type are detailed in Developing a new Node class For an overview of every type of Node available in SignalFlow, see the Node Reference Library \u2192 Next: Node playback","title":"Nodes"},{"location":"node/#nodes","text":"A Node object is an audio processing unit that performs one single function. For example, a Node's role may be to synthesize a waveform, read from a buffer, or take two input Nodes and sum their values. Nodes are played and stopped by connecting them to the AudioGraph A node has one or more audio-rate inputs , which can be modulated by other nodes Some nodes can be triggered with trigger inputs \u2014 for example, to restart playback, or set the position of an envelope Some nodes can be used to play back the contents of buffer inputs , or can use buffer data as a source of modulation The output of multiple nodes can be combined and modulated with use of the standard Python operators ( + , - , * , % , etc) The output of a node can be mono (single-channel) or multichannel A Node's status and output can be examined by querying its properties Some Nodes generate unpredictable stochastic output , which can be controlled via its internal random number generator Details of how to create a new Node type are detailed in Developing a new Node class For an overview of every type of Node available in SignalFlow, see the Node Reference Library \u2192 Next: Node playback","title":"Nodes"},{"location":"node/developing/","text":"Nodes Developing new Node classes See CONTRIBUTING.md","title":"Developing a new Node class"},{"location":"node/developing/#nodes","text":"","title":"Nodes"},{"location":"node/developing/#developing-new-node-classes","text":"See CONTRIBUTING.md","title":"Developing new Node classes"},{"location":"node/inputs/","text":"Nodes Node inputs A node has three different classes of input: Audio-rate inputs : Takes the output of another node as an input, for continuous modulation of synthesis parameters Trigger inputs : Used to trigger discrete control events \u2014 for example, restarting buffer playback Buffer inputs : Used to pass the contents of an audio buffer to a node \u2014 for example, as a source of audio samples, or an envelope shape Audio-rate inputs Virtually every node has one or more audio-rate inputs. Put simply, an audio-rate input is the output of another node . Let's look at a short example: lfo = SineLFO () signal = SquareOscillator ( frequency = 200 , width = lfo ) In this case, we are passing the output of a SineLFO as the pulse width of a SquareOscillator . This is an audio-rate input. Although it's not obvious, the frequency parameter is also an audio-rate input. Any constant value (such as the 200 here) is behind the scenes implemented as a Constant node, which continuously outputs the value at an audio rate. All audio-rate inputs can be modified just like a normal Python property. For example: signal . frequency = TriangleOscillator ( 0.5 , 100 , 1000 ) Variable input nodes Some nodes have a variable number of inputs, which can change over the Node's lifetime. For example, Sum() takes an arbitrary number of input Nodes, and generates an output which is the sum of all of its inputs. For variable-input nodes such as this, audio-rate inputs are added with add_input() , and can be removed with remove_input() . a = Constant ( 1 ) b = Constant ( 2 ) c = Constant ( 3 ) sum = Sum () sum . add_input ( a ) sum . add_input ( b ) sum . add_input ( c ) # sum will now generate an output of 6.0 It is possible to check whether a Node object takes variable inputs by querying node.has_variable_inputs . Triggers When working with sequencing and timing, it is often useful be able to trigger discrete events within a node. This is where trigger inputs come in handy. There are two different ways to handle trigger inputs: by calling the trigger() method on a Node by passing a Node to an input that corresponds to an audio-rate trigger Calling trigger() To generate trigger events at arbitrary times, call node.trigger() . For example: freq_env = Line ( 10000 , 100 , 0.5 ) sine = SineOscillator ( freq_env ) sine . play () while True : freq_env . trigger () graph . wait ( 1 ) This is useful because it can be done outside the audio thread. For example, trigger() could be called each time a MIDI note event is received. The trigger() method takes an optional name parameter, which is used by Node classes containing more than one type of trigger. This example uses the set_position trigger of BufferPlayer to seek to a new location in the sample every second. buffer = Buffer ( \"../audio/stereo-count.wav\" ) player = BufferPlayer ( buffer , loop = True ) player . play () while True : player . trigger ( \"set_position\" , random_uniform ( 0 , buffer . duration )) graph . wait ( 1 ) Note Because the trigger method happens outside the audio thread, it will take effect at the start of the next audio block. This means that, if you are running at 44.1kHz with an audio buffer size of 1024 samples, this could introduce a latency of up to 1024/44100 = 0.023s . For time-critical events like drum triggers, this can be minimised by reducing the hardware output buffer size . This constraint also means that only one event can be triggered per audio block. To trigger events at a faster rate than the hardware buffer size allows, see Audio-rate triggers below. Audio-rate triggers It is often desirable to trigger events using the audio-rate output of another Node object as a source of trigger events, to give sample-level precision in timing. Most nodes that support trigger inputs can also be triggered by a corresponding audio-rate input. Triggers happen at zero-crossings \u2014 that is, when the output of the node passes above zero (i.e., from <= 0 to >0 ). For example, to create a clock with an oscillating tempo to re-trigger buffer playback: clock = Impulse ( SineLFO ( 0.2 , 1 , 10 )) buffer = Buffer ( \"../audio/stereo-count.wav\" ) player = BufferPlayer ( buffer , loop = True , clock = clock ) player . play () This can be used to your advantage with the boolean operator nodes. on_the_right = MouseX () > 0.5 envelope = ASREnvelope ( 0 , 0 , 0.5 , clock = on_the_right ) square = SquareOscillator ( 100 ) output = envelope * square * 0.1 output . play () TODO: Should the name of the trigger() event always be identical to the trigger input name? So clock for envelopes, buffer player, etc...? Buffer inputs The third type of input supported by nodes is the buffer . Nodes often take buffer inputs as sources of audio samples. They are also useful as sources of envelope shape data (for example, to shape the grains of a Granulator), or general control data (for example, recording motion patterns from a MouseX input). buffer = Buffer ( \"../audio/stereo-count.wav\" ) player = BufferPlayer ( buffer , loop = True ) \u2192 Next: Operators","title":"Inputs"},{"location":"node/inputs/#nodes","text":"","title":"Nodes"},{"location":"node/inputs/#node-inputs","text":"A node has three different classes of input: Audio-rate inputs : Takes the output of another node as an input, for continuous modulation of synthesis parameters Trigger inputs : Used to trigger discrete control events \u2014 for example, restarting buffer playback Buffer inputs : Used to pass the contents of an audio buffer to a node \u2014 for example, as a source of audio samples, or an envelope shape","title":"Node inputs"},{"location":"node/inputs/#audio-rate-inputs","text":"Virtually every node has one or more audio-rate inputs. Put simply, an audio-rate input is the output of another node . Let's look at a short example: lfo = SineLFO () signal = SquareOscillator ( frequency = 200 , width = lfo ) In this case, we are passing the output of a SineLFO as the pulse width of a SquareOscillator . This is an audio-rate input. Although it's not obvious, the frequency parameter is also an audio-rate input. Any constant value (such as the 200 here) is behind the scenes implemented as a Constant node, which continuously outputs the value at an audio rate. All audio-rate inputs can be modified just like a normal Python property. For example: signal . frequency = TriangleOscillator ( 0.5 , 100 , 1000 )","title":"Audio-rate inputs"},{"location":"node/inputs/#variable-input-nodes","text":"Some nodes have a variable number of inputs, which can change over the Node's lifetime. For example, Sum() takes an arbitrary number of input Nodes, and generates an output which is the sum of all of its inputs. For variable-input nodes such as this, audio-rate inputs are added with add_input() , and can be removed with remove_input() . a = Constant ( 1 ) b = Constant ( 2 ) c = Constant ( 3 ) sum = Sum () sum . add_input ( a ) sum . add_input ( b ) sum . add_input ( c ) # sum will now generate an output of 6.0 It is possible to check whether a Node object takes variable inputs by querying node.has_variable_inputs .","title":"Variable input nodes"},{"location":"node/inputs/#triggers","text":"When working with sequencing and timing, it is often useful be able to trigger discrete events within a node. This is where trigger inputs come in handy. There are two different ways to handle trigger inputs: by calling the trigger() method on a Node by passing a Node to an input that corresponds to an audio-rate trigger","title":"Triggers"},{"location":"node/inputs/#calling-trigger","text":"To generate trigger events at arbitrary times, call node.trigger() . For example: freq_env = Line ( 10000 , 100 , 0.5 ) sine = SineOscillator ( freq_env ) sine . play () while True : freq_env . trigger () graph . wait ( 1 ) This is useful because it can be done outside the audio thread. For example, trigger() could be called each time a MIDI note event is received. The trigger() method takes an optional name parameter, which is used by Node classes containing more than one type of trigger. This example uses the set_position trigger of BufferPlayer to seek to a new location in the sample every second. buffer = Buffer ( \"../audio/stereo-count.wav\" ) player = BufferPlayer ( buffer , loop = True ) player . play () while True : player . trigger ( \"set_position\" , random_uniform ( 0 , buffer . duration )) graph . wait ( 1 ) Note Because the trigger method happens outside the audio thread, it will take effect at the start of the next audio block. This means that, if you are running at 44.1kHz with an audio buffer size of 1024 samples, this could introduce a latency of up to 1024/44100 = 0.023s . For time-critical events like drum triggers, this can be minimised by reducing the hardware output buffer size . This constraint also means that only one event can be triggered per audio block. To trigger events at a faster rate than the hardware buffer size allows, see Audio-rate triggers below.","title":"Calling trigger()"},{"location":"node/inputs/#audio-rate-triggers","text":"It is often desirable to trigger events using the audio-rate output of another Node object as a source of trigger events, to give sample-level precision in timing. Most nodes that support trigger inputs can also be triggered by a corresponding audio-rate input. Triggers happen at zero-crossings \u2014 that is, when the output of the node passes above zero (i.e., from <= 0 to >0 ). For example, to create a clock with an oscillating tempo to re-trigger buffer playback: clock = Impulse ( SineLFO ( 0.2 , 1 , 10 )) buffer = Buffer ( \"../audio/stereo-count.wav\" ) player = BufferPlayer ( buffer , loop = True , clock = clock ) player . play () This can be used to your advantage with the boolean operator nodes. on_the_right = MouseX () > 0.5 envelope = ASREnvelope ( 0 , 0 , 0.5 , clock = on_the_right ) square = SquareOscillator ( 100 ) output = envelope * square * 0.1 output . play () TODO: Should the name of the trigger() event always be identical to the trigger input name? So clock for envelopes, buffer player, etc...?","title":"Audio-rate triggers"},{"location":"node/inputs/#buffer-inputs","text":"The third type of input supported by nodes is the buffer . Nodes often take buffer inputs as sources of audio samples. They are also useful as sources of envelope shape data (for example, to shape the grains of a Granulator), or general control data (for example, recording motion patterns from a MouseX input). buffer = Buffer ( \"../audio/stereo-count.wav\" ) player = BufferPlayer ( buffer , loop = True ) \u2192 Next: Operators","title":"Buffer inputs"},{"location":"node/library/","text":"Node reference library analysis CrossCorrelate (input=nullptr, buffer=nullptr, hop_size=0) OnsetDetector (input=0.0, threshold=2.0, min_interval=0.1) VampAnalysis (input=0.0, plugin_id=\"vamp-example-plugins:spectralcentroid:linearcentroid\") buffer BeatCutter (buffer=nullptr, segment_count=8, stutter_probability=0.0, stutter_count=1, jump_probability=0.0, duty_cycle=1.0, rate=1.0, segment_rate=1.0) BufferPlayer (buffer=nullptr, rate=1.0, loop=0, start_time=nullptr, end_time=nullptr, clock=nullptr) BufferRecorder (buffer=nullptr, input=0.0, feedback=0.0, loop=false) FeedbackBufferReader (buffer=nullptr) FeedbackBufferWriter (buffer=nullptr, input=0.0, delay_time=0.1) GrainSegments (buffer=nullptr, clock=0, target=0, offsets={}, values={}, durations={}) Granulator (buffer=nullptr, clock=0, pos=0, duration=0.1, pan=0.0, rate=1.0, max_grains=2048) SegmentPlayer (buffer=nullptr, onsets={}) control MouseX () MouseY () MouseDown (button_index=0) envelope ADSREnvelope (attack=0.1, decay=0.1, sustain=0.5, release=0.1, gate=0) ASREnvelope (attack=0.1, sustain=0.5, release=0.1, curve=1.0, clock=nullptr) Envelope (levels=std::vector<NodeRef> ( ), times=std::vector<NodeRef> ( ), curves=std::vector<NodeRef> ( ), clock=nullptr, loop=false) Line (from=0.0, to=1.0, time=1.0, loop=0, clock=nullptr) EnvelopeRect (sustain=1.0, clock=nullptr) fft FFTContinuousPhaseVocoder (input=nullptr, rate=1.0) FFTConvolve (input=nullptr, buffer=nullptr) FFT (input=0.0, fft_size=SIGNALFLOW_DEFAULT_FFT_SIZE, hop_size=SIGNALFLOW_DEFAULT_FFT_HOP_SIZE, window_size=0, do_window=true) FFTNode (fft_size=None, hop_size=None, window_size=None, do_window=None) FFTOpNode (input=nullptr) FFTFindPeaks (input=0, prominence=1, threshold=0.000001, count=SIGNALFLOW_MAX_CHANNELS, interpolate=true) IFFT (input=nullptr, do_window=false) FFTLPF (input=0, frequency=2000) FFTNoiseGate (input=0, threshold=0.5) FFTPhaseVocoder (input=nullptr) FFTTonality (input=0, level=0.5, smoothing=0.9) FFTZeroPhase (input=0) operators Add (a=0, b=0) AmplitudeToDecibels (a=0) DecibelsToAmplitude (a=0) ChannelArray () ChannelMixer (channels=1, input=0, amplitude_compensation=true) ChannelSelect (input=nullptr, offset=0, maximum=0, step=1) Equal (a=0, b=0) NotEqual (a=0, b=0) GreaterThan (a=0, b=0) GreaterThanOrEqual (a=0, b=0) LessThan (a=0, b=0) LessThanOrEqual (a=0, b=0) Modulo (a=0, b=0) Abs (a=0) If (a=0, value_if_true=0, value_if_false=0) Divide (a=1, b=1) FrequencyToMidiNote (a=0) MidiNoteToFrequency (a=0) Multiply (a=1.0, b=1.0) Pow (a=0, b=0) RoundToScale (a=0) Round (a=0) ScaleLinExp (input=0, a=0, b=1, c=1, d=10) ScaleLinLin (input=0, a=0, b=1, c=1, d=10) Subtract (a=0, b=0) Sum () Tanh (a=0) oscillators Constant (value=0) Impulse (frequency=1.0) LFO (frequency=1.0, min=0.0, max=1.0) SawLFO (frequency=1.0, min=0.0, max=1.0) SawOscillator (frequency=440) SineLFO (frequency=1.0, min=0.0, max=1.0) SineOscillator (frequency=440) SquareLFO (frequency=1.0, min=0.0, max=1.0, width=0.5) SquareOscillator (frequency=440, width=0.5) TriangleLFO (frequency=1.0, min=0.0, max=1.0) TriangleOscillator (frequency=440) Wavetable (buffer=nullptr, frequency=440, phase=0, sync=0, phase_map=nullptr) Wavetable2D (buffer=nullptr, frequency=440, crossfade=0.0, sync=0) processors Clip (input=nullptr, min=-1.0, max=1.0) processors/delays AllpassDelay (input=0.0, delaytime=0.1, feedback=0.5, maxdelaytime=0.5) CombDelay (input=0.0, delaytime=0.1, feedback=0.5, maxdelaytime=0.5) OneTapDelay (input=0.0, delaytime=0.1, maxdelaytime=0.5) Stutter (input=0.0, stutter_time=0.1, stutter_count=1, clock=nullptr, max_stutter_time=1.0) processors/distortion Resample (input=0, sample_rate=44100, bit_rate=16) SampleAndHold (input=nullptr, clock=nullptr) Squiz (input=0.0, rate=2.0, chunk_size=1) WaveShaper (input=0.0, buffer=nullptr) processors/dynamics Compressor (input=0.0, threshold=0.1, ratio=2, attack_time=0.01, release_time=0.1, sidechain=nullptr) Gate (input=0.0, threshold=0.1) Maximiser (input=0.0, ceiling=0.5, attack_time=1.0, release_time=1.0) RMS (input=0.0) processors/filters BiquadFilter (input=0.0, filter_type=SIGNALFLOW_FILTER_TYPE_LOW_PASS, cutoff=440, resonance=0.0, peak_gain=0.0) EQ (input=0.0, low_gain=1.0, mid_gain=1.0, high_gain=1.0, low_freq=500, high_freq=5000) MoogVCF (input=0.0, cutoff=200.0, resonance=0.0) SVFilter (input=0.0, filter_type=SIGNALFLOW_FILTER_TYPE_LOW_PASS, cutoff=440, resonance=0.0) processors Fold (input=nullptr, min=-1.0, max=1.0) processors/panning LinearPanner (channels=2, input=0, pan=0.0) StereoBalance (input=0, balance=0) StereoWidth (input=0, width=1) processors Smooth (input=nullptr, smooth=0.99) WetDry (dry_input=nullptr, wet_input=nullptr, wetness=0.0) Wrap (input=nullptr, min=-1.0, max=1.0) sequencing ClockDivider (clock=0, factor=1) Counter (clock=0, min=0, max=2147483647) Euclidean (clock=0, sequence_length=0, num_events=0) FlipFlop (clock=0) ImpulseSequence (sequence=std::vector<int> ( ), clock=nullptr) Index (list={}, index=0) Latch (set=0, reset=0) Sequence (sequence=std::vector<float> ( ), clock=nullptr) stochastic Logistic (chaos=3.7, frequency=0.0) PinkNoise (low_cutoff=20.0, high_cutoff=20000.0, reset=nullptr) RandomBrownian (min=-1.0, max=1.0, delta=0.01, clock=nullptr, reset=nullptr) RandomChoice (values=std::vector<float> ( ), clock=nullptr, reset=nullptr) RandomCoin (probability=0.5, clock=nullptr, reset=nullptr) RandomExponentialDist (scale=0.0, clock=nullptr, reset=nullptr) RandomExponential (min=0.001, max=1.0, clock=nullptr, reset=nullptr) RandomGaussian (mean=0.0, sigma=0.0, clock=nullptr, reset=nullptr) RandomImpulseSequence (probability=0.5, length=8, clock=nullptr, explore=nullptr, generate=nullptr, reset=nullptr) RandomImpulse (frequency=1.0, distribution=SIGNALFLOW_EVENT_DISTRIBUTION_UNIFORM, reset=nullptr) RandomUniform (min=0.0, max=1.0, clock=nullptr, reset=nullptr) StochasticNode (reset=nullptr) WhiteNoise (frequency=0.0, min=-1.0, max=1.0, interpolate=true, random_interval=true, reset=nullptr)","title":"Reference library"},{"location":"node/library/#node-reference-library","text":"","title":"Node reference library"},{"location":"node/library/#analysis","text":"CrossCorrelate (input=nullptr, buffer=nullptr, hop_size=0) OnsetDetector (input=0.0, threshold=2.0, min_interval=0.1) VampAnalysis (input=0.0, plugin_id=\"vamp-example-plugins:spectralcentroid:linearcentroid\")","title":"analysis"},{"location":"node/library/#buffer","text":"BeatCutter (buffer=nullptr, segment_count=8, stutter_probability=0.0, stutter_count=1, jump_probability=0.0, duty_cycle=1.0, rate=1.0, segment_rate=1.0) BufferPlayer (buffer=nullptr, rate=1.0, loop=0, start_time=nullptr, end_time=nullptr, clock=nullptr) BufferRecorder (buffer=nullptr, input=0.0, feedback=0.0, loop=false) FeedbackBufferReader (buffer=nullptr) FeedbackBufferWriter (buffer=nullptr, input=0.0, delay_time=0.1) GrainSegments (buffer=nullptr, clock=0, target=0, offsets={}, values={}, durations={}) Granulator (buffer=nullptr, clock=0, pos=0, duration=0.1, pan=0.0, rate=1.0, max_grains=2048) SegmentPlayer (buffer=nullptr, onsets={})","title":"buffer"},{"location":"node/library/#control","text":"MouseX () MouseY () MouseDown (button_index=0)","title":"control"},{"location":"node/library/#envelope","text":"ADSREnvelope (attack=0.1, decay=0.1, sustain=0.5, release=0.1, gate=0) ASREnvelope (attack=0.1, sustain=0.5, release=0.1, curve=1.0, clock=nullptr) Envelope (levels=std::vector<NodeRef> ( ), times=std::vector<NodeRef> ( ), curves=std::vector<NodeRef> ( ), clock=nullptr, loop=false) Line (from=0.0, to=1.0, time=1.0, loop=0, clock=nullptr) EnvelopeRect (sustain=1.0, clock=nullptr)","title":"envelope"},{"location":"node/library/#fft","text":"FFTContinuousPhaseVocoder (input=nullptr, rate=1.0) FFTConvolve (input=nullptr, buffer=nullptr) FFT (input=0.0, fft_size=SIGNALFLOW_DEFAULT_FFT_SIZE, hop_size=SIGNALFLOW_DEFAULT_FFT_HOP_SIZE, window_size=0, do_window=true) FFTNode (fft_size=None, hop_size=None, window_size=None, do_window=None) FFTOpNode (input=nullptr) FFTFindPeaks (input=0, prominence=1, threshold=0.000001, count=SIGNALFLOW_MAX_CHANNELS, interpolate=true) IFFT (input=nullptr, do_window=false) FFTLPF (input=0, frequency=2000) FFTNoiseGate (input=0, threshold=0.5) FFTPhaseVocoder (input=nullptr) FFTTonality (input=0, level=0.5, smoothing=0.9) FFTZeroPhase (input=0)","title":"fft"},{"location":"node/library/#operators","text":"Add (a=0, b=0) AmplitudeToDecibels (a=0) DecibelsToAmplitude (a=0) ChannelArray () ChannelMixer (channels=1, input=0, amplitude_compensation=true) ChannelSelect (input=nullptr, offset=0, maximum=0, step=1) Equal (a=0, b=0) NotEqual (a=0, b=0) GreaterThan (a=0, b=0) GreaterThanOrEqual (a=0, b=0) LessThan (a=0, b=0) LessThanOrEqual (a=0, b=0) Modulo (a=0, b=0) Abs (a=0) If (a=0, value_if_true=0, value_if_false=0) Divide (a=1, b=1) FrequencyToMidiNote (a=0) MidiNoteToFrequency (a=0) Multiply (a=1.0, b=1.0) Pow (a=0, b=0) RoundToScale (a=0) Round (a=0) ScaleLinExp (input=0, a=0, b=1, c=1, d=10) ScaleLinLin (input=0, a=0, b=1, c=1, d=10) Subtract (a=0, b=0) Sum () Tanh (a=0)","title":"operators"},{"location":"node/library/#oscillators","text":"Constant (value=0) Impulse (frequency=1.0) LFO (frequency=1.0, min=0.0, max=1.0) SawLFO (frequency=1.0, min=0.0, max=1.0) SawOscillator (frequency=440) SineLFO (frequency=1.0, min=0.0, max=1.0) SineOscillator (frequency=440) SquareLFO (frequency=1.0, min=0.0, max=1.0, width=0.5) SquareOscillator (frequency=440, width=0.5) TriangleLFO (frequency=1.0, min=0.0, max=1.0) TriangleOscillator (frequency=440) Wavetable (buffer=nullptr, frequency=440, phase=0, sync=0, phase_map=nullptr) Wavetable2D (buffer=nullptr, frequency=440, crossfade=0.0, sync=0)","title":"oscillators"},{"location":"node/library/#processors","text":"Clip (input=nullptr, min=-1.0, max=1.0)","title":"processors"},{"location":"node/library/#processorsdelays","text":"AllpassDelay (input=0.0, delaytime=0.1, feedback=0.5, maxdelaytime=0.5) CombDelay (input=0.0, delaytime=0.1, feedback=0.5, maxdelaytime=0.5) OneTapDelay (input=0.0, delaytime=0.1, maxdelaytime=0.5) Stutter (input=0.0, stutter_time=0.1, stutter_count=1, clock=nullptr, max_stutter_time=1.0)","title":"processors/delays"},{"location":"node/library/#processorsdistortion","text":"Resample (input=0, sample_rate=44100, bit_rate=16) SampleAndHold (input=nullptr, clock=nullptr) Squiz (input=0.0, rate=2.0, chunk_size=1) WaveShaper (input=0.0, buffer=nullptr)","title":"processors/distortion"},{"location":"node/library/#processorsdynamics","text":"Compressor (input=0.0, threshold=0.1, ratio=2, attack_time=0.01, release_time=0.1, sidechain=nullptr) Gate (input=0.0, threshold=0.1) Maximiser (input=0.0, ceiling=0.5, attack_time=1.0, release_time=1.0) RMS (input=0.0)","title":"processors/dynamics"},{"location":"node/library/#processorsfilters","text":"BiquadFilter (input=0.0, filter_type=SIGNALFLOW_FILTER_TYPE_LOW_PASS, cutoff=440, resonance=0.0, peak_gain=0.0) EQ (input=0.0, low_gain=1.0, mid_gain=1.0, high_gain=1.0, low_freq=500, high_freq=5000) MoogVCF (input=0.0, cutoff=200.0, resonance=0.0) SVFilter (input=0.0, filter_type=SIGNALFLOW_FILTER_TYPE_LOW_PASS, cutoff=440, resonance=0.0)","title":"processors/filters"},{"location":"node/library/#processors_1","text":"Fold (input=nullptr, min=-1.0, max=1.0)","title":"processors"},{"location":"node/library/#processorspanning","text":"LinearPanner (channels=2, input=0, pan=0.0) StereoBalance (input=0, balance=0) StereoWidth (input=0, width=1)","title":"processors/panning"},{"location":"node/library/#processors_2","text":"Smooth (input=nullptr, smooth=0.99) WetDry (dry_input=nullptr, wet_input=nullptr, wetness=0.0) Wrap (input=nullptr, min=-1.0, max=1.0)","title":"processors"},{"location":"node/library/#sequencing","text":"ClockDivider (clock=0, factor=1) Counter (clock=0, min=0, max=2147483647) Euclidean (clock=0, sequence_length=0, num_events=0) FlipFlop (clock=0) ImpulseSequence (sequence=std::vector<int> ( ), clock=nullptr) Index (list={}, index=0) Latch (set=0, reset=0) Sequence (sequence=std::vector<float> ( ), clock=nullptr)","title":"sequencing"},{"location":"node/library/#stochastic","text":"Logistic (chaos=3.7, frequency=0.0) PinkNoise (low_cutoff=20.0, high_cutoff=20000.0, reset=nullptr) RandomBrownian (min=-1.0, max=1.0, delta=0.01, clock=nullptr, reset=nullptr) RandomChoice (values=std::vector<float> ( ), clock=nullptr, reset=nullptr) RandomCoin (probability=0.5, clock=nullptr, reset=nullptr) RandomExponentialDist (scale=0.0, clock=nullptr, reset=nullptr) RandomExponential (min=0.001, max=1.0, clock=nullptr, reset=nullptr) RandomGaussian (mean=0.0, sigma=0.0, clock=nullptr, reset=nullptr) RandomImpulseSequence (probability=0.5, length=8, clock=nullptr, explore=nullptr, generate=nullptr, reset=nullptr) RandomImpulse (frequency=1.0, distribution=SIGNALFLOW_EVENT_DISTRIBUTION_UNIFORM, reset=nullptr) RandomUniform (min=0.0, max=1.0, clock=nullptr, reset=nullptr) StochasticNode (reset=nullptr) WhiteNoise (frequency=0.0, min=-1.0, max=1.0, interpolate=true, random_interval=true, reset=nullptr)","title":"stochastic"},{"location":"node/multichannel/","text":"Nodes Multichannel nodes When passing a value to audio-rate input of a Node, the signal is by default monophonic (single-channel). For example, SquareOscillator(440) generates a 1-channel output. It is possible to generate multi-channel output by passing an array of values in the place of a constant. For example, SquareOscillator([440, 880]) generates stereo output with a different frequency in the L and R channels. There is no limit to the number of channels that can be generated by a node. For example, SquareOscillator(list(100 + 50 * n for n in range(100))) will create a node with 100-channel output, each with its own frequency. >>> sq = SquareOscillator ([ 100 + 50 * n for n in range ( 100 )]) >>> print ( sq . num_output_channels ) 100 Automatic upmixing There are generally multiple inputs connected to a node, which may themselves have differing number of channels. For example, SquareOscillator(frequency=[100, 200, 300, 400, 500], width=0.7) has a 5-channel input and a 1-channel input. In cases like this, the output of the nodes with fewer channels is upmixed to match the higher-channel inputs. Upmixing here means simply duplicating the output until it reaches the desired number of channels. In the above case, the width input will be upmixed to generate 5 channels, all containing 0.7 . If width were a stereo input with L and R channels, the output would be tiled, alternating between the channels. Each frame of stereo input would then be upmixed to contain [L, R, L, R, L] , where L and R are the samples corresponding to the L and R channels. The key rule is that, for nodes that support upmixing, the output signal has as many channels as the input signal with the highest channel count . This process percolates through the signal chain. For example: SquareOscillator ( frequency = SineLFO ([ 1 , 3 , 5 ], min = 440 , max = 880 ), width = SawLFO ([ 0.5 , 0.6 ], min = 0.25 , max = 0.75 )) The min and max inputs of the frequency LFO would be upmixed to 3 channels each The min and max inputs of the width LFO would be upmixed to 2 channels each Then, the output of the width node would be upmixed from 2 to 3 channels Nodes with fixed input/output channels Some nodes have immutable numbers of input/output channels. For example: StereoPanner has 1 input channel and 2 output channels StereoBalance has 2 input channels and 2 output channels ChannelMixer has an arbitrary number of input channels, but a fixed, user-specified number of output channels Even Nodes that do not have an obvious input (e.g. BufferPlayer ) have input channels, for modulation inputs (for example, modulating the rate of the buffer). When two nodes are connected together with incompatible channel counts (for example, connecting a StereoBalance into a StereoMixer ), an InvalidChannelCountException will be raised. The Channel* node classes There are a number of Node subclasses dedicated to channel handling. ChannelArray : Concatenates the channels of multiple nodes, so that calling ChannelMix with nodes of N and M channels will produce an output of N + M channels. ChannelMixer : Reduces or expands the number of channels by evenly spreading the audio across the output channels. ChannelSelect : Selects sub-channels of the input, either individually or by group. Querying channel subsets with the index operator Single channels of a multi-channel node can be accessed using the index [] operator. For example: square = SquareOscillator ([ 440 , 441 , 442 , 443 ]) output = square [ 0 ] # output now contains a mono output, with a frequency of 440Hz. Slice syntax can be used to query multiple subchannels: square = SquareOscillator ([ 440 , 441 , 442 , 880 ]) output = square [ 0 : 2 ] # now contains a two-channel square wave \u2192 Next: Status and properties","title":"Multichannel"},{"location":"node/multichannel/#nodes","text":"","title":"Nodes"},{"location":"node/multichannel/#multichannel-nodes","text":"When passing a value to audio-rate input of a Node, the signal is by default monophonic (single-channel). For example, SquareOscillator(440) generates a 1-channel output. It is possible to generate multi-channel output by passing an array of values in the place of a constant. For example, SquareOscillator([440, 880]) generates stereo output with a different frequency in the L and R channels. There is no limit to the number of channels that can be generated by a node. For example, SquareOscillator(list(100 + 50 * n for n in range(100))) will create a node with 100-channel output, each with its own frequency. >>> sq = SquareOscillator ([ 100 + 50 * n for n in range ( 100 )]) >>> print ( sq . num_output_channels ) 100","title":"Multichannel nodes"},{"location":"node/multichannel/#automatic-upmixing","text":"There are generally multiple inputs connected to a node, which may themselves have differing number of channels. For example, SquareOscillator(frequency=[100, 200, 300, 400, 500], width=0.7) has a 5-channel input and a 1-channel input. In cases like this, the output of the nodes with fewer channels is upmixed to match the higher-channel inputs. Upmixing here means simply duplicating the output until it reaches the desired number of channels. In the above case, the width input will be upmixed to generate 5 channels, all containing 0.7 . If width were a stereo input with L and R channels, the output would be tiled, alternating between the channels. Each frame of stereo input would then be upmixed to contain [L, R, L, R, L] , where L and R are the samples corresponding to the L and R channels. The key rule is that, for nodes that support upmixing, the output signal has as many channels as the input signal with the highest channel count . This process percolates through the signal chain. For example: SquareOscillator ( frequency = SineLFO ([ 1 , 3 , 5 ], min = 440 , max = 880 ), width = SawLFO ([ 0.5 , 0.6 ], min = 0.25 , max = 0.75 )) The min and max inputs of the frequency LFO would be upmixed to 3 channels each The min and max inputs of the width LFO would be upmixed to 2 channels each Then, the output of the width node would be upmixed from 2 to 3 channels","title":"Automatic upmixing"},{"location":"node/multichannel/#nodes-with-fixed-inputoutput-channels","text":"Some nodes have immutable numbers of input/output channels. For example: StereoPanner has 1 input channel and 2 output channels StereoBalance has 2 input channels and 2 output channels ChannelMixer has an arbitrary number of input channels, but a fixed, user-specified number of output channels Even Nodes that do not have an obvious input (e.g. BufferPlayer ) have input channels, for modulation inputs (for example, modulating the rate of the buffer). When two nodes are connected together with incompatible channel counts (for example, connecting a StereoBalance into a StereoMixer ), an InvalidChannelCountException will be raised.","title":"Nodes with fixed input/output channels"},{"location":"node/multichannel/#the-channel-node-classes","text":"There are a number of Node subclasses dedicated to channel handling. ChannelArray : Concatenates the channels of multiple nodes, so that calling ChannelMix with nodes of N and M channels will produce an output of N + M channels. ChannelMixer : Reduces or expands the number of channels by evenly spreading the audio across the output channels. ChannelSelect : Selects sub-channels of the input, either individually or by group.","title":"The Channel* node classes"},{"location":"node/multichannel/#querying-channel-subsets-with-the-index-operator","text":"Single channels of a multi-channel node can be accessed using the index [] operator. For example: square = SquareOscillator ([ 440 , 441 , 442 , 443 ]) output = square [ 0 ] # output now contains a mono output, with a frequency of 440Hz. Slice syntax can be used to query multiple subchannels: square = SquareOscillator ([ 440 , 441 , 442 , 880 ]) output = square [ 0 : 2 ] # now contains a two-channel square wave \u2192 Next: Status and properties","title":"Querying channel subsets with the index operator"},{"location":"node/operators/","text":"Nodes Node operators Arithmetic The output of multiple nodes can be combined using Python's mathematical operators. For example, to sum two sine waves together to create harmonics, use the + operator: output = SineOscillator ( 440 ) + SineOscillator ( 880 ) output . play () To modulate the amplitude of one node with another, use the * operator: sine = SineOscillator ( 440 ) envelope = ASREnvelope ( 0.1 , 1 , 0.1 ) output = sine * envelope You can use constant values in place of Node objects: sine = SineOscillator ( 440 ) attenuated = sine * 0.5 Operators can be chained together in the normal way: # Create an envelope that rises from 0.5 to 1.0 and back to 0.5 env = ( ASREnvelope ( 0.1 , 1 , 0.1 ) * 0.5 ) + 0.5 Behind the scenes, these operators are actually creating composites of Node subclasses. The last example could alternatively be written as: Add ( Multiply ( ASREnvelope ( 0.1 , 1 , 0.1 ), 0.5 ), 0.5 ) Comparison Comparison operators can also be used to compare two Node output values, generating a binary (1/0) output. For example: # Generates an output of 1 when the sinusoid is above 0, and 0 otherwise SineOscillator ( 440 ) > 0 This can then be used as an input to other nodes. The below will generate a half-wave-rectified sine signal (that is, a sine wave with all negative values set to zero). sine = SineOscillator ( 440 ) rectified = sine * ( sine > 0 ) Index of operators Below is a full list of operators supported by SignalFlow. Arithmetic operators Operator Node class + Add - Subtract * Multiply / Divide ** Power % Modulo Comparison operators Operator Node class == Equal != NotEqual < LessThan <= LessThanOrEqual > GreaterThan >= GreaterThanOrEqual \u2192 Next: Multichannel","title":"Operators"},{"location":"node/operators/#nodes","text":"","title":"Nodes"},{"location":"node/operators/#node-operators","text":"","title":"Node operators"},{"location":"node/operators/#arithmetic","text":"The output of multiple nodes can be combined using Python's mathematical operators. For example, to sum two sine waves together to create harmonics, use the + operator: output = SineOscillator ( 440 ) + SineOscillator ( 880 ) output . play () To modulate the amplitude of one node with another, use the * operator: sine = SineOscillator ( 440 ) envelope = ASREnvelope ( 0.1 , 1 , 0.1 ) output = sine * envelope You can use constant values in place of Node objects: sine = SineOscillator ( 440 ) attenuated = sine * 0.5 Operators can be chained together in the normal way: # Create an envelope that rises from 0.5 to 1.0 and back to 0.5 env = ( ASREnvelope ( 0.1 , 1 , 0.1 ) * 0.5 ) + 0.5 Behind the scenes, these operators are actually creating composites of Node subclasses. The last example could alternatively be written as: Add ( Multiply ( ASREnvelope ( 0.1 , 1 , 0.1 ), 0.5 ), 0.5 )","title":"Arithmetic"},{"location":"node/operators/#comparison","text":"Comparison operators can also be used to compare two Node output values, generating a binary (1/0) output. For example: # Generates an output of 1 when the sinusoid is above 0, and 0 otherwise SineOscillator ( 440 ) > 0 This can then be used as an input to other nodes. The below will generate a half-wave-rectified sine signal (that is, a sine wave with all negative values set to zero). sine = SineOscillator ( 440 ) rectified = sine * ( sine > 0 )","title":"Comparison"},{"location":"node/operators/#index-of-operators","text":"Below is a full list of operators supported by SignalFlow.","title":"Index of operators"},{"location":"node/operators/#arithmetic-operators","text":"Operator Node class + Add - Subtract * Multiply / Divide ** Power % Modulo","title":"Arithmetic operators"},{"location":"node/operators/#comparison-operators","text":"Operator Node class == Equal != NotEqual < LessThan <= LessThanOrEqual > GreaterThan >= GreaterThanOrEqual \u2192 Next: Multichannel","title":"Comparison operators"},{"location":"node/playback/","text":"Nodes Playing and stopping a node Starting playback To start a node playing, simply call the play() method: graph = AudioGraph () node = SineOscillator ( 440 ) node . play () This connects the node to the output endpoint of the current global AudioGraph . The next time the graph processes a block of samples, the graph's output node then calls upon the sine oscillator to generate a block. It is important to remember that playing a node means \"connecting it to the graph\". For this reason, it is not possible to play the same node more than once, as it is already connected to the graph. To play multiples of a particular Node type, simply create and play multiple instances. Connecting a Node to another Node's input It is often the case that you want to connect a Node to the input of another Node for playback, rather than simply wiring it to the output of a graph -- for example, to pass an oscillator through a processor. In this case, you do not need to call play() (which means \"connect this node to the graph\"). Instead, it is sufficient to simply connect the Node to the input of another Node that is already playing. For example: # create and begin playback of a variable input summer, passed through a filter sum = Sum () flt = SVFilter ( sum , \"low_pass\" , 200 ) flt . play () Now, let's create an oscillator. Observe that connecting the oscillator to the filter's input begins playback immediately. square = SquareOscillator ( 100 ) sum . add_input ( square ) Stopping playback To stop a node playing: node . stop () This disconnects the node from the output device that it is connected to. \u2192 Next: Inputs","title":"Playback"},{"location":"node/playback/#nodes","text":"","title":"Nodes"},{"location":"node/playback/#playing-and-stopping-a-node","text":"","title":"Playing and stopping a node"},{"location":"node/playback/#starting-playback","text":"To start a node playing, simply call the play() method: graph = AudioGraph () node = SineOscillator ( 440 ) node . play () This connects the node to the output endpoint of the current global AudioGraph . The next time the graph processes a block of samples, the graph's output node then calls upon the sine oscillator to generate a block. It is important to remember that playing a node means \"connecting it to the graph\". For this reason, it is not possible to play the same node more than once, as it is already connected to the graph. To play multiples of a particular Node type, simply create and play multiple instances.","title":"Starting playback"},{"location":"node/playback/#connecting-a-node-to-another-nodes-input","text":"It is often the case that you want to connect a Node to the input of another Node for playback, rather than simply wiring it to the output of a graph -- for example, to pass an oscillator through a processor. In this case, you do not need to call play() (which means \"connect this node to the graph\"). Instead, it is sufficient to simply connect the Node to the input of another Node that is already playing. For example: # create and begin playback of a variable input summer, passed through a filter sum = Sum () flt = SVFilter ( sum , \"low_pass\" , 200 ) flt . play () Now, let's create an oscillator. Observe that connecting the oscillator to the filter's input begins playback immediately. square = SquareOscillator ( 100 ) sum . add_input ( square )","title":"Connecting a Node to another Node's input"},{"location":"node/playback/#stopping-playback","text":"To stop a node playing: node . stop () This disconnects the node from the output device that it is connected to. \u2192 Next: Inputs","title":"Stopping playback"},{"location":"node/properties/","text":"Nodes Node properties A Node has a number of read-only properties which can be used to query its status at a given moment in time. Property Type Description name str Short alphanumeric string that identifies the type of node (for example, asr-envelope ) num_output_channels int The number of output channels that the node generates. num_input_channels int The number of input channels that the node takes. Note that most nodes have matches_input_channels set, meaning that their num_input_channels will be automatically increased according to their inputs. To learn more, see Nodes: Multichannel . matches_input_channels bool Whether the node automatically increases its num_input_channels based on its inputs. To learn more, see Nodes: Multichannel . has_variable_inputs bool Whether the node supports an arbitrary number of audio-rate inputs output_buffer numpy.ndarray Contains the Node's most recent audio output, in float32 samples. The buffer is indexed by channel x frame , so to obtain the 32nd sample in the first channel, query: node.output_buffer[0][31] . inputs dict A dict containing all of the Node 's audio-rate inputs. Note that buffer inputs are not currently included within this dict. state int The Node's current playback state, which can be one of SIGNALFLOW_NODE_STATE_ACTIVE and SIGNALFLOW_NODE_STATE_STOPPED . The STOPPED state only applies to those nodes which have a finite duration (e.g. ASREnvelope , or BufferPlayer with looping disabled) and have reached the end of playback. Nodes continue to have a state of ACTIVE whether or not they are connected to the graph. patch Patch Indicates the Patch that the node is part of, or None if the Node does not belong to a Patch. Monitoring a node's output To monitor the output of a node, call node.poll(num_seconds) , where num_seconds is the interval between messages. This will print the last sample generated by the node to stdout . In the case of multichannel nodes, only the first channel's value is printed. >>> a = Counter(Impulse(1)) >>> a.poll(1) >>> a.play() counter: 0.00000 counter: 1.00000 counter: 2.00000 To stop polling a node, call node.poll(0) . \u2192 Next: Stochastic nodes","title":"Status and properties"},{"location":"node/properties/#nodes","text":"","title":"Nodes"},{"location":"node/properties/#node-properties","text":"A Node has a number of read-only properties which can be used to query its status at a given moment in time. Property Type Description name str Short alphanumeric string that identifies the type of node (for example, asr-envelope ) num_output_channels int The number of output channels that the node generates. num_input_channels int The number of input channels that the node takes. Note that most nodes have matches_input_channels set, meaning that their num_input_channels will be automatically increased according to their inputs. To learn more, see Nodes: Multichannel . matches_input_channels bool Whether the node automatically increases its num_input_channels based on its inputs. To learn more, see Nodes: Multichannel . has_variable_inputs bool Whether the node supports an arbitrary number of audio-rate inputs output_buffer numpy.ndarray Contains the Node's most recent audio output, in float32 samples. The buffer is indexed by channel x frame , so to obtain the 32nd sample in the first channel, query: node.output_buffer[0][31] . inputs dict A dict containing all of the Node 's audio-rate inputs. Note that buffer inputs are not currently included within this dict. state int The Node's current playback state, which can be one of SIGNALFLOW_NODE_STATE_ACTIVE and SIGNALFLOW_NODE_STATE_STOPPED . The STOPPED state only applies to those nodes which have a finite duration (e.g. ASREnvelope , or BufferPlayer with looping disabled) and have reached the end of playback. Nodes continue to have a state of ACTIVE whether or not they are connected to the graph. patch Patch Indicates the Patch that the node is part of, or None if the Node does not belong to a Patch.","title":"Node properties"},{"location":"node/properties/#monitoring-a-nodes-output","text":"To monitor the output of a node, call node.poll(num_seconds) , where num_seconds is the interval between messages. This will print the last sample generated by the node to stdout . In the case of multichannel nodes, only the first channel's value is printed. >>> a = Counter(Impulse(1)) >>> a.poll(1) >>> a.play() counter: 0.00000 counter: 1.00000 counter: 2.00000 To stop polling a node, call node.poll(0) . \u2192 Next: Stochastic nodes","title":"Monitoring a node's output"},{"location":"node/stochastic/","text":"Nodes Chance and stochastic nodes SignalFlow has a number of stochastic nodes, which make use of a pseudo-random number generator (RNG) to produce unpredictable output values. Each object of these StochasticNode subclasses stores its own RNG. By default, the RNG is seeded with a random value, so that each run will generate a different set of outputs. However, to create a repeatable pseudo-random output, the seed of the node's RNG can be set to a known value: >>> r = RandomUniform ( 0 , 1 ) >>> r . process ( 1024 ) >>> r . output_buffer [ 0 ][: 4 ] array ([ 0.48836085 , 0.64326525 , 0.79819506 , 0.8489549 ], dtype = float32 ) >>> r . set_seed ( 123 ) >>> r . process ( 1024 ) >>> r . output_buffer [ 0 ][: 4 ] array ([ 0.7129553 , 0.42847094 , 0.6908848 , 0.7191503 ], dtype = float32 ) >>> r . set_seed ( 123 ) >>> r . process ( 1024 ) >>> r . output_buffer [ 0 ][: 4 ] array ([ 0.7129553 , 0.42847094 , 0.6908848 , 0.7191503 ], dtype = float32 ) Note the identical sequences generated after repeatedly setting the seed to a known value. Warning Calling node.process() is generally not good practice, as it does not recursively process all of the node's inputs (unlike when a node is embedded within an AudioGraph, which correctly handles recursion and cyclical loops). Please use at your peril! \u2192 Next: Node reference library","title":"Stochastic nodes"},{"location":"node/stochastic/#nodes","text":"","title":"Nodes"},{"location":"node/stochastic/#chance-and-stochastic-nodes","text":"SignalFlow has a number of stochastic nodes, which make use of a pseudo-random number generator (RNG) to produce unpredictable output values. Each object of these StochasticNode subclasses stores its own RNG. By default, the RNG is seeded with a random value, so that each run will generate a different set of outputs. However, to create a repeatable pseudo-random output, the seed of the node's RNG can be set to a known value: >>> r = RandomUniform ( 0 , 1 ) >>> r . process ( 1024 ) >>> r . output_buffer [ 0 ][: 4 ] array ([ 0.48836085 , 0.64326525 , 0.79819506 , 0.8489549 ], dtype = float32 ) >>> r . set_seed ( 123 ) >>> r . process ( 1024 ) >>> r . output_buffer [ 0 ][: 4 ] array ([ 0.7129553 , 0.42847094 , 0.6908848 , 0.7191503 ], dtype = float32 ) >>> r . set_seed ( 123 ) >>> r . process ( 1024 ) >>> r . output_buffer [ 0 ][: 4 ] array ([ 0.7129553 , 0.42847094 , 0.6908848 , 0.7191503 ], dtype = float32 ) Note the identical sequences generated after repeatedly setting the seed to a known value. Warning Calling node.process() is generally not good practice, as it does not recursively process all of the node's inputs (unlike when a node is embedded within an AudioGraph, which correctly handles recursion and cyclical loops). Please use at your peril! \u2192 Next: Node reference library","title":"Chance and stochastic nodes"},{"location":"patch/","text":"Patch Warning This documentation is a work-in-progress and may have sections that are missing or incomplete. A Patch represents a connected group of Nodes , analogous to a synthesizer. Defining patches makes it easy to create higher-level structures, which can then be reused and instantiated with a single line of code, in much the same way as a Node. Behind the scenes, the structure of a Patch is encapsulated by a PatchSpec , a template which can be instantiated or serialised to a JSON file for later use. A Patch structure is defined either by declaring a Patch subclass or with a JSON specification file Play and stop a Patch by connecting it to the AudioGraph or the input of another Patch or Node Similar to nodes, a Patch can be modulated by audio-rate inputs , triggered by trigger inputs , and access sample data via buffer inputs The outputs of Patches can be altered or combined by normal Python operators The status of a Patch can be queried via its properties Patches can be exported and imported to JSON The auto-free mechanism allows Patches to automatically stop and free their memory after playback is complete \u2192 Next: Defining a Patch","title":"Patches"},{"location":"patch/#patch","text":"Warning This documentation is a work-in-progress and may have sections that are missing or incomplete. A Patch represents a connected group of Nodes , analogous to a synthesizer. Defining patches makes it easy to create higher-level structures, which can then be reused and instantiated with a single line of code, in much the same way as a Node. Behind the scenes, the structure of a Patch is encapsulated by a PatchSpec , a template which can be instantiated or serialised to a JSON file for later use. A Patch structure is defined either by declaring a Patch subclass or with a JSON specification file Play and stop a Patch by connecting it to the AudioGraph or the input of another Patch or Node Similar to nodes, a Patch can be modulated by audio-rate inputs , triggered by trigger inputs , and access sample data via buffer inputs The outputs of Patches can be altered or combined by normal Python operators The status of a Patch can be queried via its properties Patches can be exported and imported to JSON The auto-free mechanism allows Patches to automatically stop and free their memory after playback is complete \u2192 Next: Defining a Patch","title":"Patch"},{"location":"patch/auto-free/","text":"Patch Auto-free and memory management Auto-free.","title":"Auto-free and memory management"},{"location":"patch/auto-free/#patch","text":"","title":"Patch"},{"location":"patch/auto-free/#auto-free-and-memory-management","text":"Auto-free.","title":"Auto-free and memory management"},{"location":"patch/defining/","text":"Patch Defining a Patch A Patch is made up of a connected network of Nodes, together with a set of properties that determine how the Patch can be controlled. There are two general ways to define the structure of a Patch: Create a new class that subclasses Patch . In general, this is the recommended approach for defining new Patches. Create a JSON file that can be loaded as a PatchSpec , which describes the structure of a patch Creating a Patch subclass The quickest and most intuitive way to define a Patch is by subclassing the Patch class itself. Let's look at an example. class Bleep ( Patch ): def __init__ ( self , frequency = 880 , duration = 0.1 ): super () . __init__ () frequency = self . add_input ( \"frequency\" , frequency ) duration = self . add_input ( \"duration\" , duration ) sine = SineOscillator ( frequency ) env = ASREnvelope ( 0.001 , duration , 0.001 ) output = sine * env self . set_output ( output ) self . set_auto_free ( True ) In the above example: At the very start of the __init__ function, super().__init__() must be called to initialise the Patch and its storage. This is vital! Without it, your program will crash. Two audio-rate input parameters are defined. The add_input() method is used to define them as inputs of the Patch , which can then be subsequently modulated. Note that the add_input() method returns a reference to the frequency node, which then acts as a pointer to the input node. self.set_output() is used to define the Patch's output. A Patch can only have one single output. Finally, self.set_auto_free() is used to automatically stop and free the Patch after playback of the envelope is completed. More about auto-free... You can now instantiate a Bleep object in just the same way as you would instantiate and play a Node: b = Bleep(frequency=440, duration=0.2) b.play() If you query graph.status after playback has finished, you should see that the Patch is automatically freed and the number of nodes returns to 0. Creating a PatchSpec from JSON The structure of a Patch is described by a PatchSpec , which can in turn be imported/exported in the JSON text-based data interchange format. For information on loading or saving PatchSpecs as JSON, see Exporting and importing patches . \u2192 Next: Playing and stopping a Patch","title":"Defining a Patch"},{"location":"patch/defining/#patch","text":"","title":"Patch"},{"location":"patch/defining/#defining-a-patch","text":"A Patch is made up of a connected network of Nodes, together with a set of properties that determine how the Patch can be controlled. There are two general ways to define the structure of a Patch: Create a new class that subclasses Patch . In general, this is the recommended approach for defining new Patches. Create a JSON file that can be loaded as a PatchSpec , which describes the structure of a patch","title":"Defining a Patch"},{"location":"patch/defining/#creating-a-patch-subclass","text":"The quickest and most intuitive way to define a Patch is by subclassing the Patch class itself. Let's look at an example. class Bleep ( Patch ): def __init__ ( self , frequency = 880 , duration = 0.1 ): super () . __init__ () frequency = self . add_input ( \"frequency\" , frequency ) duration = self . add_input ( \"duration\" , duration ) sine = SineOscillator ( frequency ) env = ASREnvelope ( 0.001 , duration , 0.001 ) output = sine * env self . set_output ( output ) self . set_auto_free ( True ) In the above example: At the very start of the __init__ function, super().__init__() must be called to initialise the Patch and its storage. This is vital! Without it, your program will crash. Two audio-rate input parameters are defined. The add_input() method is used to define them as inputs of the Patch , which can then be subsequently modulated. Note that the add_input() method returns a reference to the frequency node, which then acts as a pointer to the input node. self.set_output() is used to define the Patch's output. A Patch can only have one single output. Finally, self.set_auto_free() is used to automatically stop and free the Patch after playback of the envelope is completed. More about auto-free... You can now instantiate a Bleep object in just the same way as you would instantiate and play a Node: b = Bleep(frequency=440, duration=0.2) b.play() If you query graph.status after playback has finished, you should see that the Patch is automatically freed and the number of nodes returns to 0.","title":"Creating a Patch subclass"},{"location":"patch/defining/#creating-a-patchspec-from-json","text":"The structure of a Patch is described by a PatchSpec , which can in turn be imported/exported in the JSON text-based data interchange format. For information on loading or saving PatchSpecs as JSON, see Exporting and importing patches . \u2192 Next: Playing and stopping a Patch","title":"Creating a PatchSpec from JSON"},{"location":"patch/exporting/","text":"Patch Exporting and importing patches A Patch can be exported or imported. \u2192 Next: Auto-free and memory management","title":"Exporting and importing patches"},{"location":"patch/exporting/#patch","text":"","title":"Patch"},{"location":"patch/exporting/#exporting-and-importing-patches","text":"A Patch can be exported or imported. \u2192 Next: Auto-free and memory management","title":"Exporting and importing patches"},{"location":"patch/inputs/","text":"Patch Patch inputs Just like a Node , a Patch supports three different classes of input: Audio-rate inputs : Takes the output of another Node or Patch as an input, for continuous modulation of synthesis parameters Trigger inputs : Used to trigger discrete control events \u2014 for example, restarting buffer playback Buffer inputs : Used to pass the contents of an audio buffer to a patch \u2014 for example, as a source of audio samples, or an envelope shape Audio-rate inputs A Patch supports any number of user-defined named inputs, which can be used to modulate the nodes within the patch. Each input must be defined by calling add_input() when the Patch is first defined, with an optional default value. Info Note that Patches do not yet support variable inputs . When a Patch is playing, the value of its inputs can be set using patch.set_input() : class Bloop ( Patch ): def __init__ ( self , frequency = 880 , duration = 0.1 ): super () . __init__ () frequency = self . add_input ( \"frequency\" , frequency ) sine = SineOscillator ( frequency ) self . set_output ( sine ) self . set_auto_free ( True ) bloop = Bloop () bloop . play () ... bloop . set_input ( \"frequency\" , 100 ) Info Note that Patches do not yet support setting inputs with Python properties (e.g. patch.prop_name = 123 ), as is possible with node inputs . Triggers When defining a Patch , it is possible to define which Node should receive trigger() events sent to the Patch. This is done with patch.set_trigger_node() : class Hat ( Patch ): def __init__ ( self , duration = 0.1 ): super () . __init__ () duration = self . add_input ( \"duration\" , duration ) noise = WhiteNoise () env = ASREnvelope ( 0.0001 , 0.0 , duration , curve = 2 ) output = noise * env self . set_trigger_node ( env ) self . set_output ( output ) h = Hat () h . play () ... h . trigger () # triggers a hit, resetting the ASREnvelope to its start point This can be used to create a Patch that stays connected to the AudioGraph and can be retriggered to play a hit. Info Note that Patches only presently support trigger events directed to a single node within the patch, and cannot route triggers to multiple different nodes. Buffer inputs Buffer inputs can be declared at define time by calling self.add_buffer_input() . Similar to add_input , the return value is a placeholder Buffer that can be used wherever you would normally pass a Buffer : class WobblyPlayer ( Patch ): def __init__ ( self , buffer ): super () . __init__ () buffer = self . add_buffer_input ( \"buffer\" , buffer ) rate = SineLFO ( 0.2 , 0.5 , 1.5 ) player = BufferPlayer ( buffer , rate = rate , loop = True ) self . set_output ( player ) buffer = Buffer ( \"examples/audio/stereo-count.wav\" ) player = WobblyPlayer ( buffer ) player . play () The buffer can then be replaced at runtime by calling set_input() : player . set_input ( \"buffer\" , another_buffer ) \u2192 Next: Operators","title":"Inputs"},{"location":"patch/inputs/#patch","text":"","title":"Patch"},{"location":"patch/inputs/#patch-inputs","text":"Just like a Node , a Patch supports three different classes of input: Audio-rate inputs : Takes the output of another Node or Patch as an input, for continuous modulation of synthesis parameters Trigger inputs : Used to trigger discrete control events \u2014 for example, restarting buffer playback Buffer inputs : Used to pass the contents of an audio buffer to a patch \u2014 for example, as a source of audio samples, or an envelope shape","title":"Patch inputs"},{"location":"patch/inputs/#audio-rate-inputs","text":"A Patch supports any number of user-defined named inputs, which can be used to modulate the nodes within the patch. Each input must be defined by calling add_input() when the Patch is first defined, with an optional default value. Info Note that Patches do not yet support variable inputs . When a Patch is playing, the value of its inputs can be set using patch.set_input() : class Bloop ( Patch ): def __init__ ( self , frequency = 880 , duration = 0.1 ): super () . __init__ () frequency = self . add_input ( \"frequency\" , frequency ) sine = SineOscillator ( frequency ) self . set_output ( sine ) self . set_auto_free ( True ) bloop = Bloop () bloop . play () ... bloop . set_input ( \"frequency\" , 100 ) Info Note that Patches do not yet support setting inputs with Python properties (e.g. patch.prop_name = 123 ), as is possible with node inputs .","title":"Audio-rate inputs"},{"location":"patch/inputs/#triggers","text":"When defining a Patch , it is possible to define which Node should receive trigger() events sent to the Patch. This is done with patch.set_trigger_node() : class Hat ( Patch ): def __init__ ( self , duration = 0.1 ): super () . __init__ () duration = self . add_input ( \"duration\" , duration ) noise = WhiteNoise () env = ASREnvelope ( 0.0001 , 0.0 , duration , curve = 2 ) output = noise * env self . set_trigger_node ( env ) self . set_output ( output ) h = Hat () h . play () ... h . trigger () # triggers a hit, resetting the ASREnvelope to its start point This can be used to create a Patch that stays connected to the AudioGraph and can be retriggered to play a hit. Info Note that Patches only presently support trigger events directed to a single node within the patch, and cannot route triggers to multiple different nodes.","title":"Triggers"},{"location":"patch/inputs/#buffer-inputs","text":"Buffer inputs can be declared at define time by calling self.add_buffer_input() . Similar to add_input , the return value is a placeholder Buffer that can be used wherever you would normally pass a Buffer : class WobblyPlayer ( Patch ): def __init__ ( self , buffer ): super () . __init__ () buffer = self . add_buffer_input ( \"buffer\" , buffer ) rate = SineLFO ( 0.2 , 0.5 , 1.5 ) player = BufferPlayer ( buffer , rate = rate , loop = True ) self . set_output ( player ) buffer = Buffer ( \"examples/audio/stereo-count.wav\" ) player = WobblyPlayer ( buffer ) player . play () The buffer can then be replaced at runtime by calling set_input() : player . set_input ( \"buffer\" , another_buffer ) \u2192 Next: Operators","title":"Buffer inputs"},{"location":"patch/operators/","text":"Patch Operators The output of a Patch can be amplified, attenuated, combined, modulated and compared using Python operators, in much the same way as Node : patch = Patch ( patch_spec ) output = patch * 0.5 For a full list of the operators that can be applied to a Patch , see Node operators . \u2192 Next: Patch properties","title":"Operators"},{"location":"patch/operators/#patch","text":"","title":"Patch"},{"location":"patch/operators/#operators","text":"The output of a Patch can be amplified, attenuated, combined, modulated and compared using Python operators, in much the same way as Node : patch = Patch ( patch_spec ) output = patch * 0.5 For a full list of the operators that can be applied to a Patch , see Node operators . \u2192 Next: Patch properties","title":"Operators"},{"location":"patch/playback/","text":"Patch Playing a Patch Once a Patch has been defined or imported, it can be instantiated in two different ways depending on how it was defined: From a Patch subclass From a PatchSpec From a Patch subclass The simplest way to instantiate a Patch is by defining it as a Patch subclass, and then instantiating it in the same way as a Node. class Hat ( Patch ): def __init__ ( self , duration = 0.1 ): super () . __init__ () duration = self . add_input ( \"duration\" , duration ) noise = WhiteNoise () env = ASREnvelope ( 0.0001 , 0.0 , duration , curve = 2 ) output = noise * env self . set_output ( output ) self . set_auto_free ( True ) hat = Hat () hat . play () Once a Patch has finished, its state changes to SIGNALFLOW_PATCH_STATE_STOPPED . Just as with nodes, it is important to remember that playing a patch means \"connecting it to the graph\". For this reason, it is not possible to play the same patch more than once, as it is already connected to the graph. To play multiples of a particular Patch type, simply create and play multiple instances. From a PatchSpec Once a PatchSpec has been created or imported, it can be played by instantiating a Patch with the PatchSpec as an argument: patch = Patch ( patch_spec ) patch . play () Connecting a Patch to another Patch's input A Patch can be connected to the input of another Patch (or Node), in exactly the same way described in Connecting a Node to another Node's input . Once you have got to grips with this paradigm, it becomes simple to build up sophisticated processing graphs by abstracting complex functionality within individual Patch objects, and connecting them to one another. Stopping a Patch As in Node playback , stopping a Patch disconnects it from the AudioGraph. Patches with auto-free are automatically stopped when their lifetimes ends. Patches with an unlimited lifespan must be stopped manually, with: patch . stop () This disconnects the Patch from its output. \u2192 Next: Patch inputs","title":"Playback"},{"location":"patch/playback/#patch","text":"","title":"Patch"},{"location":"patch/playback/#playing-a-patch","text":"Once a Patch has been defined or imported, it can be instantiated in two different ways depending on how it was defined: From a Patch subclass From a PatchSpec","title":"Playing a Patch"},{"location":"patch/playback/#from-a-patch-subclass","text":"The simplest way to instantiate a Patch is by defining it as a Patch subclass, and then instantiating it in the same way as a Node. class Hat ( Patch ): def __init__ ( self , duration = 0.1 ): super () . __init__ () duration = self . add_input ( \"duration\" , duration ) noise = WhiteNoise () env = ASREnvelope ( 0.0001 , 0.0 , duration , curve = 2 ) output = noise * env self . set_output ( output ) self . set_auto_free ( True ) hat = Hat () hat . play () Once a Patch has finished, its state changes to SIGNALFLOW_PATCH_STATE_STOPPED . Just as with nodes, it is important to remember that playing a patch means \"connecting it to the graph\". For this reason, it is not possible to play the same patch more than once, as it is already connected to the graph. To play multiples of a particular Patch type, simply create and play multiple instances.","title":"From a Patch subclass"},{"location":"patch/playback/#from-a-patchspec","text":"Once a PatchSpec has been created or imported, it can be played by instantiating a Patch with the PatchSpec as an argument: patch = Patch ( patch_spec ) patch . play ()","title":"From a PatchSpec"},{"location":"patch/playback/#connecting-a-patch-to-another-patchs-input","text":"A Patch can be connected to the input of another Patch (or Node), in exactly the same way described in Connecting a Node to another Node's input . Once you have got to grips with this paradigm, it becomes simple to build up sophisticated processing graphs by abstracting complex functionality within individual Patch objects, and connecting them to one another.","title":"Connecting a Patch to another Patch's input"},{"location":"patch/playback/#stopping-a-patch","text":"As in Node playback , stopping a Patch disconnects it from the AudioGraph. Patches with auto-free are automatically stopped when their lifetimes ends. Patches with an unlimited lifespan must be stopped manually, with: patch . stop () This disconnects the Patch from its output. \u2192 Next: Patch inputs","title":"Stopping a Patch"},{"location":"patch/properties/","text":"Patch Patch properties Property Type Description nodes list A list of all of the Node objects that make up this Patch inputs dict A dict of key-value pairs corresponding to all of the (audio rate) inputs within the Patch state int The Patch's current playback state, which can be SIGNALFLOW_PATCH_STATE_ACTIVE or SIGNALFLOW_PATCH_STATE_STOPPED . graph AudioGraph A reference to the AudioGraph that the Patch is part of \u2192 Next: Exporting and importing patches","title":"Properties"},{"location":"patch/properties/#patch","text":"","title":"Patch"},{"location":"patch/properties/#patch-properties","text":"Property Type Description nodes list A list of all of the Node objects that make up this Patch inputs dict A dict of key-value pairs corresponding to all of the (audio rate) inputs within the Patch state int The Patch's current playback state, which can be SIGNALFLOW_PATCH_STATE_ACTIVE or SIGNALFLOW_PATCH_STATE_STOPPED . graph AudioGraph A reference to the AudioGraph that the Patch is part of \u2192 Next: Exporting and importing patches","title":"Patch properties"}]}